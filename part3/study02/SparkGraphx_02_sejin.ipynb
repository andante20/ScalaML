{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Spark and GraphX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache Spark Graph Processing - chapter1(복습), 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFiles:  \n",
    "https://www.packtpub.com/books/content/support/21578  \n",
    "chapter1, 2  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.dropbox.com/s/v3iprxv5fjxrn8o/data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "본 발표자료에 사용된 자료는 모두 \"./data/filename\"으로 설정되어있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 선행학습 !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.slideshare.net/sanghoonlee982/spark-overview-20141106  \n",
    "Spark overview 이상훈(SK C&C)_스파크 사용자 모임_20141106 \n",
    "\n",
    "http://www.slideshare.net/sangwookimme/graphx  \n",
    "스사모 테크톡 (김상우,정향민) - GraphX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 아직 분석환경이 갖추어 지지않아 Spark 를 공부할 준비가 안되셨다면~ : Let's start with Docker~!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "docker run -d -p 9999:8888 -e GRANT_SUDO=yes --name psy_spark jupyter/all-spark-notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "jupyter노트북은 8888 port로 기본 설정되어있습니다. 자신이 사용하고 싶은 port로 연결해줍니다.\n",
    "저는 이미 다른 컨테이너에서 8888포트를 사용하고 있어 9999로 설정하였습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자신의 서버 ip 또는 docker ip :9999 로 Jupyter notebook을 실행합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 지난시간 복습 ch1 (발표는 될 수록 일찍 합시다 :) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a tiny social network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import the GraphX and RDD module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.graphx._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.rdd.RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    @ dataset : 두 개의 CSV파일   \n",
    "    ./data/people.csv  \n",
    "    ./data/links.csv  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val people = sc.textFile(\"./data/people.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[2] at textFile at <console>:18"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people: org.apache.spark.rdd.RDD[String]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val links = sc.textFile(\"./data/links.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[4] at textFile at <console>:18"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links : org.apache.spark.rdd.RDD[String]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The property graph"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "class Graph[VD, ED] {\n",
    "    val vertices : VertexRDD[VD]\n",
    "    val edges : EdgeRDD[ED, VD]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming RDDs to VertexRDD and EdgeRDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### graph를 구축하기 위한 Person 클래스 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "case class Person(name: String, age: Int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val peopleRDD: RDD[(VertexId, Person)] = people map { line =>\n",
    "    val row = line split ','\n",
    "    (row(0).toInt, Person(row(1), row(2).toInt))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type Connection = String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val linksRDD: RDD[Edge[Connection]] = links map {line =>\n",
    "         val row = line split ','\n",
    "         Edge(row(0).toInt, row(1).toInt, row(2))\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val tinySocial: Graph[Person, Connection] =\n",
    "       Graph(peopleRDD, linksRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing graph operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tinySocial.vertices.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(Edge(1,2,friend), Edge(1,3,sister), Edge(2,4,brother), Edge(3,2,boss), Edge(4,5,client), Edge(1,9,friend), Edge(6,7,cousin), Edge(7,9,coworker), Edge(8,9,father))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tinySocial.edges.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val profLinks: List[Connection] = List(\"coworker\", \"boss\", \"employee\",\"client\", \"supplier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Charlie is a boss of Bob\n",
      "Dave is a client of Eve\n",
      "George is a coworker of Ivy\n"
     ]
    }
   ],
   "source": [
    "val profNetwork =\n",
    "   tinySocial.edges.filter{ case Edge(_,_,link) =>\n",
    "   profLinks.contains(link)}\n",
    "   for {\n",
    "     Edge(src, dst, link) <- profNetwork.collect()\n",
    "     srcName = (peopleRDD.filter{case (id, person) => id == src}\n",
    "   first)._2.name\n",
    "     dstName = (peopleRDD.filter{case (id, person) => id == dst}\n",
    "   first)._2.name\n",
    "   } println(srcName + \" is a \" + link + \" of \" + dstName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same expression : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```    \n",
    "    tinySocial.subgraph(profLinks contains _.attr).  \n",
    "        triplets.foreach(t => println(t.srcAttr.name + \" is a \" + t.attr + \" of \" + t.dstAttr.name))  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result - triplet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```\n",
    "    Triplet view   \n",
    "    EdgeTriplet -- 3-tuple  \n",
    "    ((VertexId, Person),(VertiexId, Person),(Connection))  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chapter2 : Building and Exploring Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- map their components to vertices or nodes  \n",
    "- map the interactions between the individual components to edges or links  \n",
    "- how graphs are stored and represented in GraphX  \n",
    "- language of graph theory, and the basic characteristics of graphs  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 이번 챕터에서는"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "    다양한 포맷의 데이터를 불러와서 여러가지 그래프를 만들어봅시다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Load data and build Spark graphs in many ways   \n",
    "• Use the join operator to mix external data into existing graphs  \n",
    "• Build bipartite graphs and multigraphs  \n",
    "• Explore graphs and compute their basic statistics  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network datasets : real-world datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "    지난 챕터에서는 toy example이었다면, 이번 챕터에서는 세 가지의 실제 데이터셋을 다룹니다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* e-mail communication networks\n",
    "* food flavor netwmork\n",
    "* social ego networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 첫번째 데이터셋 : The communication network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "email communication graph : history of e-mails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The original dataset was released by William Cohen at CMU, which can be downloaded from https://www.cs.cmu.edu/~./enron/. A detailed description of the complete dataset was done by Klimmt and Yang, 2004. A cleaner version of the dataset,\n",
    "which we use here, is provided by Leskovec et al., 2009, and can be obtained\n",
    "from https://snap.stanford.edu/data/email-Enron.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "canonical example of a directed graph,  \n",
    "as each e-mail links a source node to the destination node  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "database of e-mails generated by 158 employees of the Enron Corporation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 두번째 데이터셋 : Flavor networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ingredient-compound network는 다음 논문에서 소개되었다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Ingredient-compound network, introduced by Ahn et al., 2011  (http://yongyeol.com/)  \n",
    "http://yongyeol.com/papers/ahn-flavornet-2011.pdf  \n",
    "http://www.nature.com/articles/srep00196  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ingredient-compound network로 부터 flavor network 또한 만들어진다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The flavor network can also help food scientists or amateur cooks create new recipes.   \n",
    "The datasets that we will use consist of ingredient-compound data and the recipes collected from http://www.epicurious.com/, allrecipes.com, and http://www.menupan.com/.   \n",
    "The datasets are available at http://yongyeol.com/2011/12/15/paper-flavor-network.html.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "food ingredient : 식품재료, chemical compound : 화학성분\n",
    "* ingredient- compound network : 화학성분이 식품재료안에 존재할 때 link가 연결된다. (이번 챕터에서 다룬다)\n",
    "* flavor network : 식품재료(ingredient) pair 가 하나 이상의 화학성분을 공유할 때  link가 연결된다. (챕터4에서 다룬다)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://www.nature.com/article-assets/npg/srep/2011/111215/srep00196/images_hires/w926/srep00196-f1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://www.nature.com/article-assets/npg/srep/2011/111215/srep00196/images_hires/m685/srep00196-f2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 세번째 데이터셋 : Social ego networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "collection of social ego networks from Google+  \n",
    "collected by (McAuley and Leskovec, 2012)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset includes the user profiles, their circles, and their ego networks and can be downloaded from Stanford's SNAP project website at http://snap.stanford.edu/data/egonets-Gplus.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph builders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GraphX에는 property graph를 만드는 4가지 function이 있습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Graph factory method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first one is the Graph factory method that we have already seen in the previous chapter. It is defined in the apply method of the companion object called Graph, which is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    def apply[VD, ED](\n",
    "            vertices: RDD[(VertexId, VD)],\n",
    "            edges: RDD[Edge[ED]],\n",
    "            defaultVertexAttr: VD = null)\n",
    "            : Graph[VD, ED]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "두 RDD collections 인 RDD[(VertexId, VD)] 과 RDD[Edge[ED]] 를 parameter로 받아  각각 vertices와 edges로 Graph[VD, ED]를 만든다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. edgeListFile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "또 하나의 흔한 경우는, 원본 데이터셋이 edge만을 표한하는 경우이다. 이 경우는 GraphX에서 제공하는 GraphLoader.edgeListFile 함수가 GraphLoader에서 정의된다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    def edgeListFile(\n",
    "             sc: SparkContext,\n",
    "             path: String,\n",
    "             canonicalOrientation: Boolean = false,\n",
    "             minEdgePartitions: Int = 1)\n",
    "             : Graph[Int, Int]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "edge의 리스트를 포함하는 파일을 path를 받아서, 각 line이 source ID, destinationID의 2개의 integer로 graph의 edge를 표현한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. fromEdges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GraphLoader.edgeListFile 과 비슷하게, Graph.fromEdges라는 이름의 세번째 function은 RDD[Edge[ED]]로부터 그래프를 생성하도록 한다. edgeRDD에서 특정하는 vertexID파라미터를 사용하여 vertice를 만든다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     def fromEdges[VD, ED](\n",
    "          edges: RDD[Edge[ED]],\n",
    "    ￼      defaultValue: VD)\n",
    "    : Graph[VD, ED]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. fromEdgeTuples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막 graph builder function인 Graph.fromEdgeTuples 는 RDD[(VertexId, VertexId)]의 집합인 edge tuples의 RDD하나만으로 graph를 만든다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "    def fromEdgeTuples[VD](\n",
    "             rawEdges: RDD[(VertexId, VertexId)],\n",
    "             defaultValue: VD,\n",
    "             uniqueEdges: Option[PartitionStrategy] = None)\n",
    "       : Graph[VD, Int]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자, 그럼 다음 세가지 종류의 graph를 만들어 봅시다!  \n",
    "1. a directed email communication network  \n",
    "2. a bipartite graph of ingredient-compound connections  \n",
    "3. a multigraph using the previous graph builders.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: the Enron email network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building directed graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.graphx._\n",
    "import org.apache.spark.rdd._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터셋을 불러옵니다. 이 파일은 employee들간의 email communications의 adjacency list입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GraphLoader.edgeListFile method로 파일을 pass 합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val emailGraph = GraphLoader.edgeListFile(sc, \"./data/emailEnron.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.graphx.impl.GraphImpl@614894ac"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emailGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GraphLoader.edgeListFile method는 항상 int type의 vertex와 edge속성을 가지는 graph 객체를 반환합니다. graph의 처음 5개의 vertex와 edge를 확인해봅시다. (take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array((18624,1), (32196,1), (32432,1), (9166,1), (7608,1))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emailGraph.vertices.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(Edge(0,1,1), Edge(1,0,1), Edge(1,2,1), Edge(1,3,1), Edge(1,4,1))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emailGraph.edges.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GraphX에서 모든 edge들은 만드시 directed되어야 합니다. 그래서 non-direced or bidirectional graph를 표현하기 위해서, 각 connected pair를 양 방향으로 연결할 수 있습니다. 19021 node가 incoming, outgoing link 양 쪽을 갖는 노드임을 확인할 수 있다. 먼저 19021이 communication하는 destination node를 수집한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(696, 4232, 6811, 8315, 26007)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emailGraph.edges.filter(_.srcId == 19021).map(_.dstId).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "같은 node들이 19021에 대한 incoming edge를 위한 source node이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(696, 4232, 6811, 8315, 26007)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emailGraph.edges.filter(_.dstId == 19021).map(_.srcId).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 예제 한 가지를 이용해 끝까지 해보기 위해, 책과 순서를 조금 바꾸었습니다~!\n",
    "30page부터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the degrees of the network nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-degree and out-degree of the Enron email network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enron email network에 대하여, node보다 link가 약 10배 더 많음을 확인할 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "367662"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emailGraph.numEdges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36692"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emailGraph.numVertices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "직원들의 in-degree와 out-degree는 정확하게 동일한 bi-directed email graph 이며, 이는 average degree를 통해 확인할 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.020222391802028"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emailGraph.inDegrees.map(_._2).sum / emailGraph.numVertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.020222391802028"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emailGraph.outDegrees.map(_._2).sum / emailGraph.numVertices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "만약 우리가 가장 많은 수의 사람에게 이메일을 보낸 사람을 찾으려면 다음 max function을 정의하여 사용할 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max(a: (VertexId, Int), b: (VertexId, Int)): (VertexId, Int) = {\n",
    "     if (a._2 > b._2) a else b\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5038,1383)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emailGraph.outDegrees.reduce(max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 사람은 hub역할을 하는 사람일 것이다. 비슷하게 min function으로 사람을 찾을 수 있다.   \n",
    "독립된 그룹들을 찾으려면 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11211"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emailGraph.outDegrees.filter(_._2 <= 1).count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단 한 사람으로부터만 이메일을 받는 많은 직원들이 있으며 이는 아마도 조직의 보스이거나 인사팀 일 것이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Flavor network : food ingredient-compound network.  \n",
    "#### 데이터셋 : ingr_info.tsv, comp_info.tsv, ingr_comp. tsv  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a bipartite graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "어떤 경우에는 시스템을 bipartite graph의 관점에서 보는 것이 유용할 때가 있다. bipartite graph는 node가 두 셋트로 구성되어 있다. 같은 집합 안의 node들은 서로 연결 될 수 없고 다른 집합에만 연결하여 pair가 될 수 있다.  예제는 ingredient-compound network이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    food ingredient : 식품재료\n",
    "    chemical compound : 화학성분\n",
    "    * ingredient- compound network : 화학성분이 식품재료안에 존재할 때 link가 연결된다. \n",
    "    * flavor network : 식품재료(ingredient)쌍을 화학성분을 공유할 때 연결하여 link를 만든다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 이번 챕터에서는 ingredient-compound network 를, 챕터4에서는 ingredient-compound network로 부터 flavor network를 만들 것이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scala.io.Source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "첫번째 파일을 food ingredient(식품재료)에 대해, 두번째 파일은 compound(화학성분)의 정보가 들어있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# id\tingredient name\tcategory\n",
      "0\tmagnolia_tripetala\tflower\n",
      "1\tcalyptranthes_parriculata\tplant\n",
      "2\tchamaecyparis_pisifera_oil\tplant derivative\n",
      "3\tmackerel\tfish/seafood\n",
      "4\tmimusops_elengi_flower\tflower\n",
      "5\thyssop\therb\n"
     ]
    }
   ],
   "source": [
    "Source.fromFile(\"./data/ingr_info.tsv\").getLines().\n",
    "      take(7).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# id\tCompound name\tCAS number\n",
      "0\tjasmone\t488-10-8\n",
      "1\t5-methylhexanoic_acid\t628-46-6\n",
      "2\tl-glutamine\t56-85-9\n",
      "3\t1-methyl-3-methoxy-4-isopropylbenzene\t1076-56-8\n",
      "4\tmethyl-3-phenylpropionate\t103-25-3\n",
      "5\t3-mercapto-2-methylpentan-1-ol_(racemic)\t227456-27-1\n"
     ]
    }
   ],
   "source": [
    "Source.fromFile(\"./data/comp_info.tsv\").getLines().\n",
    "take(7).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "세번째 파일은 ingredient와 compound간의 adjacency list정보를가지고 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ingredient id\tcompound id\n",
      "1392\t906\n",
      "1259\t861\n",
      "1079\t673\n",
      "22\t906\n",
      "103\t906\n",
      "1005\t906\n"
     ]
    }
   ],
   "source": [
    "Source.fromFile(\"./data/ingr_comp.tsv\").getLines().\n",
    "take(7).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bipartite graph를 만들기 위하여, case class - Ingredient, Compound를 만들고, Scala inheritance를 이용하여 두 class를 상속하는 FNNode class를 만들었다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FNNode(val name: String)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "case class Ingredient(override val name: String, category: String) extends FNNode(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "case class Compound(override val name: String, cas: String) extends FNNode(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 이후에는, 우리가 Compound, Ingredient 객체를 불러와서 하나의 RDD[FNNode] collection을 만든다. 이 부분은 data wrangling이 필요하다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val ingredients: RDD[(VertexId, FNNode)] =\n",
    "sc.textFile(\"./data/ingr_info.tsv\").\n",
    "     filter(! _.startsWith(\"#\")).\n",
    "     map {line =>\n",
    "            val row = line split '\\t'\n",
    "            (row(0).toInt, Ingredient(row(1), row(2)))\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val compounds: RDD[(VertexId, FNNode)] =\n",
    "   sc.textFile(\"./data/comp_info.tsv\").\n",
    "         filter(! _.startsWith(\"#\")).\n",
    "         map {line =>\n",
    "                val row = line split '\\t'\n",
    "                (10000L + row(0).toInt, Compound(row(1), row(2)))\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val links: RDD[Edge[Int]] =\n",
    "     sc.textFile(\"./data/ingr_comp.tsv\").\n",
    "        filter(! _.startsWith(\"#\")).\n",
    "        map {line =>\n",
    "           val row = line split '\\t'\n",
    "           Edge(row(0).toInt, 10000L + row(1).toInt, 1)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "node 집합 두 개를 하나의 RDD로 붙여주고 Graph() factory method로 RDD link에 적용한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val nodes = ingredients ++ compounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val foodNetwork = Graph(nodes, links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 이제 ingredient-compound graph를 살펴보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def showTriplet(t: EdgeTriplet[FNNode,Int]): String = \"The ingredient \" ++ t.srcAttr.name ++ \" contains \" ++ t.dstAttr.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1 : foodNetwork 이부분부터 에러납니다. 총 5줄  : 집단지성의 힘을 보여주세요!!! by sejin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.SparkException\n",
       "Message: Job aborted due to stage failure: Task 2 in stage 81.0 failed 1 times, most recent failure: Lost task 2.0 in stage 81.0 (TID 114, localhost): java.io.InvalidClassException: $line191.$read$$iwC$$iwC$Ingredient; no valid constructor\n",
       "\tat java.io.ObjectStreamClass$ExceptionInfo.newInvalidClassException(ObjectStreamClass.java:150)\n",
       "\tat java.io.ObjectStreamClass.checkDeserialize(ObjectStreamClass.java:768)\n",
       "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1772)\n",
       "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)\n",
       "\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)\n",
       "\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:72)\n",
       "\tat org.apache.spark.serializer.DeserializationStream.readValue(Serializer.scala:171)\n",
       "\tat org.apache.spark.serializer.DeserializationStream$$anon$2.getNext(Serializer.scala:201)\n",
       "\tat org.apache.spark.serializer.DeserializationStream$$anon$2.getNext(Serializer.scala:198)\n",
       "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)\n",
       "\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)\n",
       "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n",
       "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)\n",
       "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)\n",
       "\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n",
       "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
       "\tat org.apache.spark.graphx.impl.ShippableVertexPartition$.apply(ShippableVertexPartition.scala:60)\n",
       "\tat org.apache.spark.graphx.VertexRDD$$anonfun$2.apply(VertexRDD.scala:328)\n",
       "\tat org.apache.spark.graphx.VertexRDD$$anonfun$2.apply(VertexRDD.scala:325)\n",
       "\tat org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:99)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)\n",
       "\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:262)\n",
       "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:264)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:88)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n",
       "\tat java.lang.Thread.run(Thread.java:745)\n",
       "\n",
       "Driver stacktrace:\n",
       "StackTrace: org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283)\n",
       "org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271)\n",
       "org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270)\n",
       "scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
       "scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n",
       "org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270)\n",
       "org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)\n",
       "org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)\n",
       "scala.Option.foreach(Option.scala:236)\n",
       "org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697)\n",
       "org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496)\n",
       "org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458)\n",
       "org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447)\n",
       "org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
       "org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567)\n",
       "org.apache.spark.SparkContext.runJob(SparkContext.scala:1822)\n",
       "org.apache.spark.SparkContext.runJob(SparkContext.scala:1835)\n",
       "org.apache.spark.SparkContext.runJob(SparkContext.scala:1848)\n",
       "org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1298)\n",
       "org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)\n",
       "org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)\n",
       "org.apache.spark.rdd.RDD.withScope(RDD.scala:306)\n",
       "org.apache.spark.rdd.RDD.take(RDD.scala:1272)\n",
       "$line199.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:59)\n",
       "$line199.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:64)\n",
       "$line199.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:66)\n",
       "$line199.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:68)\n",
       "$line199.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:70)\n",
       "$line199.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:72)\n",
       "$line199.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:74)\n",
       "$line199.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:76)\n",
       "$line199.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:78)\n",
       "$line199.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:80)\n",
       "$line199.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:82)\n",
       "$line199.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:84)\n",
       "$line199.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:86)\n",
       "$line199.$read$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:88)\n",
       "$line199.$read$$iwC$$iwC$$iwC$$iwC.<init>(<console>:90)\n",
       "$line199.$read$$iwC$$iwC$$iwC.<init>(<console>:92)\n",
       "$line199.$read$$iwC$$iwC.<init>(<console>:94)\n",
       "$line199.$read$$iwC.<init>(<console>:96)\n",
       "$line199.$read.<init>(<console>:98)\n",
       "$line199.$read$.<init>(<console>:102)\n",
       "$line199.$read$.<clinit>(<console>)\n",
       "$line199.$eval$.<init>(<console>:7)\n",
       "$line199.$eval$.<clinit>(<console>)\n",
       "$line199.$eval.$print(<console>)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n",
       "sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "java.lang.reflect.Method.invoke(Method.java:606)\n",
       "org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n",
       "org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1340)\n",
       "org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n",
       "org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n",
       "org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n",
       "com.ibm.spark.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreter.scala:354)\n",
       "com.ibm.spark.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreter.scala:349)\n",
       "com.ibm.spark.global.StreamState$.withStreams(StreamState.scala:80)\n",
       "com.ibm.spark.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1.apply(ScalaInterpreter.scala:348)\n",
       "com.ibm.spark.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1.apply(ScalaInterpreter.scala:348)\n",
       "com.ibm.spark.utils.TaskManager$$anonfun$add$2$$anon$1.run(TaskManager.scala:123)\n",
       "java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n",
       "java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n",
       "java.lang.Thread.run(Thread.java:745)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foodNetwork.triplets.take(5).foreach(showTriplet _ andThen println _)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the degrees of the network nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Degrees in the bipartite food network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bipartite ingredient-compound graph에 대하여 가장 많은 수의 compound를 갖는 food 또는 음식에 가장 빈번하게 포함되는 compound를 가지고 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.SparkException\n",
       "Message: Job aborted due to stage failure: Task 3 in stage 46.0 failed 1 times, most recent failure: Lost task 3.0 in stage 46.0 (TID 67, localhost): java.io.InvalidClassException: $line84.$read$$iwC$$iwC$Ingredient; no valid constructor\n",
       "\tat java.io.ObjectStreamClass$ExceptionInfo.newInvalidClassException(ObjectStreamClass.java:150)\n",
       "\tat java.io.ObjectStreamClass.checkDeserialize(ObjectStreamClass.java:768)\n",
       "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1772)\n",
       "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)\n",
       "\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)\n",
       "\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:72)\n",
       "\tat org.apache.spark.serializer.DeserializationStream.readValue(Serializer.scala:171)\n",
       "\tat org.apache.spark.serializer.DeserializationStream$$anon$2.getNext(Serializer.scala:201)\n",
       "\tat org.apache.spark.serializer.DeserializationStream$$anon$2.getNext(Serializer.scala:198)\n",
       "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)\n",
       "\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)\n",
       "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n",
       "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)\n",
       "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)\n",
       "\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n",
       "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
       "\tat org.apache.spark.graphx.impl.ShippableVertexPartition$.apply(ShippableVertexPartition.scala:60)\n",
       "\tat org.apache.spark.graphx.VertexRDD$$anonfun$2.apply(VertexRDD.scala:328)\n",
       "\tat org.apache.spark.graphx.VertexRDD$$anonfun$2.apply(VertexRDD.scala:325)\n",
       "\tat org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:99)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)\n",
       "\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:262)\n",
       "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:264)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:88)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n",
       "\tat java.lang.Thread.run(Thread.java:745)\n",
       "\n",
       "Driver stacktrace:\n",
       "StackTrace: org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283)\n",
       "org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271)\n",
       "org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270)\n",
       "scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
       "scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n",
       "org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270)\n",
       "org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)\n",
       "org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)\n",
       "scala.Option.foreach(Option.scala:236)\n",
       "org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697)\n",
       "org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496)\n",
       "org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458)\n",
       "org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447)\n",
       "org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
       "org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567)\n",
       "org.apache.spark.SparkContext.runJob(SparkContext.scala:1822)\n",
       "org.apache.spark.SparkContext.runJob(SparkContext.scala:1942)\n",
       "org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1003)\n",
       "org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)\n",
       "org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)\n",
       "org.apache.spark.rdd.RDD.withScope(RDD.scala:306)\n",
       "org.apache.spark.rdd.RDD.reduce(RDD.scala:985)\n",
       "$line101.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:50)\n",
       "$line101.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:55)\n",
       "$line101.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:57)\n",
       "$line101.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:59)\n",
       "$line101.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:61)\n",
       "$line101.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:63)\n",
       "$line101.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:65)\n",
       "$line101.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:67)\n",
       "$line101.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:69)\n",
       "$line101.$read$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:71)\n",
       "$line101.$read$$iwC$$iwC$$iwC$$iwC.<init>(<console>:73)\n",
       "$line101.$read$$iwC$$iwC$$iwC.<init>(<console>:75)\n",
       "$line101.$read$$iwC$$iwC.<init>(<console>:77)\n",
       "$line101.$read$$iwC.<init>(<console>:79)\n",
       "$line101.$read.<init>(<console>:81)\n",
       "$line101.$read$.<init>(<console>:85)\n",
       "$line101.$read$.<clinit>(<console>)\n",
       "$line101.$eval$.<init>(<console>:7)\n",
       "$line101.$eval$.<clinit>(<console>)\n",
       "$line101.$eval.$print(<console>)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n",
       "sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "java.lang.reflect.Method.invoke(Method.java:606)\n",
       "org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n",
       "org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1340)\n",
       "org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n",
       "org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n",
       "org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n",
       "com.ibm.spark.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreter.scala:354)\n",
       "com.ibm.spark.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreter.scala:349)\n",
       "com.ibm.spark.global.StreamState$.withStreams(StreamState.scala:80)\n",
       "com.ibm.spark.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1.apply(ScalaInterpreter.scala:348)\n",
       "com.ibm.spark.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1.apply(ScalaInterpreter.scala:348)\n",
       "com.ibm.spark.utils.TaskManager$$anonfun$add$2$$anon$1.run(TaskManager.scala:123)\n",
       "java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n",
       "java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n",
       "java.lang.Thread.run(Thread.java:745)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foodNetwork.outDegrees.reduce(max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "res: (org.apache.spark.graphx.VertexId, Int) = (908,239)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.SparkException\n",
       "Message: Job aborted due to stage failure: Task 3 in stage 51.0 failed 1 times, most recent failure: Lost task 3.0 in stage 51.0 (TID 71, localhost): java.io.InvalidClassException: $line84.$read$$iwC$$iwC$Ingredient; no valid constructor\n",
       "\tat java.io.ObjectStreamClass$ExceptionInfo.newInvalidClassException(ObjectStreamClass.java:150)\n",
       "\tat java.io.ObjectStreamClass.checkDeserialize(ObjectStreamClass.java:768)\n",
       "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1772)\n",
       "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)\n",
       "\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)\n",
       "\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:72)\n",
       "\tat org.apache.spark.serializer.DeserializationStream.readValue(Serializer.scala:171)\n",
       "\tat org.apache.spark.serializer.DeserializationStream$$anon$2.getNext(Serializer.scala:201)\n",
       "\tat org.apache.spark.serializer.DeserializationStream$$anon$2.getNext(Serializer.scala:198)\n",
       "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)\n",
       "\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)\n",
       "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n",
       "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)\n",
       "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)\n",
       "\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n",
       "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
       "\tat org.apache.spark.graphx.impl.ShippableVertexPartition$.apply(ShippableVertexPartition.scala:60)\n",
       "\tat org.apache.spark.graphx.VertexRDD$$anonfun$2.apply(VertexRDD.scala:328)\n",
       "\tat org.apache.spark.graphx.VertexRDD$$anonfun$2.apply(VertexRDD.scala:325)\n",
       "\tat org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:99)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)\n",
       "\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:262)\n",
       "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:264)\n",
       "\tat org.apache.spark.graphx.VertexRDD.compute(VertexRDD.scala:71)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:264)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:88)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n",
       "\tat java.lang.Thread.run(Thread.java:745)\n",
       "\n",
       "Driver stacktrace:\n",
       "StackTrace: org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283)\n",
       "org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271)\n",
       "org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270)\n",
       "scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
       "scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n",
       "org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270)\n",
       "org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)\n",
       "org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)\n",
       "scala.Option.foreach(Option.scala:236)\n",
       "org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697)\n",
       "org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496)\n",
       "org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458)\n",
       "org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447)\n",
       "org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
       "org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567)\n",
       "org.apache.spark.SparkContext.runJob(SparkContext.scala:1822)\n",
       "org.apache.spark.SparkContext.runJob(SparkContext.scala:1835)\n",
       "org.apache.spark.SparkContext.runJob(SparkContext.scala:1848)\n",
       "org.apache.spark.SparkContext.runJob(SparkContext.scala:1919)\n",
       "org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:905)\n",
       "org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)\n",
       "org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)\n",
       "org.apache.spark.rdd.RDD.withScope(RDD.scala:306)\n",
       "org.apache.spark.rdd.RDD.collect(RDD.scala:904)\n",
       "$line106.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:48)\n",
       "$line106.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:53)\n",
       "$line106.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:55)\n",
       "$line106.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:57)\n",
       "$line106.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:59)\n",
       "$line106.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:61)\n",
       "$line106.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:63)\n",
       "$line106.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:65)\n",
       "$line106.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:67)\n",
       "$line106.$read$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:69)\n",
       "$line106.$read$$iwC$$iwC$$iwC$$iwC.<init>(<console>:71)\n",
       "$line106.$read$$iwC$$iwC$$iwC.<init>(<console>:73)\n",
       "$line106.$read$$iwC$$iwC.<init>(<console>:75)\n",
       "$line106.$read$$iwC.<init>(<console>:77)\n",
       "$line106.$read.<init>(<console>:79)\n",
       "$line106.$read$.<init>(<console>:83)\n",
       "$line106.$read$.<clinit>(<console>)\n",
       "$line106.$eval$.<init>(<console>:7)\n",
       "$line106.$eval$.<clinit>(<console>)\n",
       "$line106.$eval.$print(<console>)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n",
       "sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "java.lang.reflect.Method.invoke(Method.java:606)\n",
       "org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n",
       "org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1340)\n",
       "org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n",
       "org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n",
       "org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n",
       "com.ibm.spark.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreter.scala:354)\n",
       "com.ibm.spark.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreter.scala:349)\n",
       "com.ibm.spark.global.StreamState$.withStreams(StreamState.scala:80)\n",
       "com.ibm.spark.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1.apply(ScalaInterpreter.scala:348)\n",
       "com.ibm.spark.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1.apply(ScalaInterpreter.scala:348)\n",
       "com.ibm.spark.utils.TaskManager$$anonfun$add$2$$anon$1.run(TaskManager.scala:123)\n",
       "java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n",
       "java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n",
       "java.lang.Thread.run(Thread.java:745)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foodNetwork.vertices.filter(_._1 == 908).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "res: Array[(org.apache.spark.graphx.VertexId, FNNode)] =\n",
    "Array((908,Ingredient(black_tea,plant derivative)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.SparkException\n",
       "Message: Job aborted due to stage failure: Task 0 in stage 54.0 failed 1 times, most recent failure: Lost task 0.0 in stage 54.0 (TID 72, localhost): java.io.InvalidClassException: $line84.$read$$iwC$$iwC$Ingredient; no valid constructor\n",
       "\tat java.io.ObjectStreamClass$ExceptionInfo.newInvalidClassException(ObjectStreamClass.java:150)\n",
       "\tat java.io.ObjectStreamClass.checkDeserialize(ObjectStreamClass.java:768)\n",
       "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1772)\n",
       "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)\n",
       "\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)\n",
       "\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:72)\n",
       "\tat org.apache.spark.serializer.DeserializationStream.readValue(Serializer.scala:171)\n",
       "\tat org.apache.spark.serializer.DeserializationStream$$anon$2.getNext(Serializer.scala:201)\n",
       "\tat org.apache.spark.serializer.DeserializationStream$$anon$2.getNext(Serializer.scala:198)\n",
       "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)\n",
       "\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)\n",
       "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n",
       "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)\n",
       "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)\n",
       "\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n",
       "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
       "\tat org.apache.spark.graphx.impl.ShippableVertexPartition$.apply(ShippableVertexPartition.scala:60)\n",
       "\tat org.apache.spark.graphx.VertexRDD$$anonfun$2.apply(VertexRDD.scala:328)\n",
       "\tat org.apache.spark.graphx.VertexRDD$$anonfun$2.apply(VertexRDD.scala:325)\n",
       "\tat org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:99)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)\n",
       "\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:262)\n",
       "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:264)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:88)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n",
       "\tat java.lang.Thread.run(Thread.java:745)\n",
       "\n",
       "Driver stacktrace:\n",
       "StackTrace: org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283)\n",
       "org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271)\n",
       "org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270)\n",
       "scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
       "scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n",
       "org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270)\n",
       "org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)\n",
       "org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)\n",
       "scala.Option.foreach(Option.scala:236)\n",
       "org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697)\n",
       "org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496)\n",
       "org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458)\n",
       "org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447)\n",
       "org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
       "org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567)\n",
       "org.apache.spark.SparkContext.runJob(SparkContext.scala:1822)\n",
       "org.apache.spark.SparkContext.runJob(SparkContext.scala:1942)\n",
       "org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1003)\n",
       "org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)\n",
       "org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)\n",
       "org.apache.spark.rdd.RDD.withScope(RDD.scala:306)\n",
       "org.apache.spark.rdd.RDD.reduce(RDD.scala:985)\n",
       "$line111.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:50)\n",
       "$line111.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:55)\n",
       "$line111.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:57)\n",
       "$line111.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:59)\n",
       "$line111.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:61)\n",
       "$line111.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:63)\n",
       "$line111.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:65)\n",
       "$line111.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:67)\n",
       "$line111.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:69)\n",
       "$line111.$read$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:71)\n",
       "$line111.$read$$iwC$$iwC$$iwC$$iwC.<init>(<console>:73)\n",
       "$line111.$read$$iwC$$iwC$$iwC.<init>(<console>:75)\n",
       "$line111.$read$$iwC$$iwC.<init>(<console>:77)\n",
       "$line111.$read$$iwC.<init>(<console>:79)\n",
       "$line111.$read.<init>(<console>:81)\n",
       "$line111.$read$.<init>(<console>:85)\n",
       "$line111.$read$.<clinit>(<console>)\n",
       "$line111.$eval$.<init>(<console>:7)\n",
       "$line111.$eval$.<clinit>(<console>)\n",
       "$line111.$eval.$print(<console>)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n",
       "sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "java.lang.reflect.Method.invoke(Method.java:606)\n",
       "org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n",
       "org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1340)\n",
       "org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n",
       "org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n",
       "org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n",
       "com.ibm.spark.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreter.scala:354)\n",
       "com.ibm.spark.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreter.scala:349)\n",
       "com.ibm.spark.global.StreamState$.withStreams(StreamState.scala:80)\n",
       "com.ibm.spark.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1.apply(ScalaInterpreter.scala:348)\n",
       "com.ibm.spark.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1.apply(ScalaInterpreter.scala:348)\n",
       "com.ibm.spark.utils.TaskManager$$anonfun$add$2$$anon$1.run(TaskManager.scala:123)\n",
       "java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n",
       "java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n",
       "java.lang.Thread.run(Thread.java:745)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foodNetwork.inDegrees.reduce(max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "res: (org.apache.spark.graphx.VertexId, Int) = (10292,299)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.SparkException\n",
       "Message: Job aborted due to stage failure: Task 0 in stage 59.0 failed 1 times, most recent failure: Lost task 0.0 in stage 59.0 (TID 76, localhost): java.io.InvalidClassException: $line84.$read$$iwC$$iwC$Ingredient; no valid constructor\n",
       "\tat java.io.ObjectStreamClass$ExceptionInfo.newInvalidClassException(ObjectStreamClass.java:150)\n",
       "\tat java.io.ObjectStreamClass.checkDeserialize(ObjectStreamClass.java:768)\n",
       "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1772)\n",
       "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)\n",
       "\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)\n",
       "\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:72)\n",
       "\tat org.apache.spark.serializer.DeserializationStream.readValue(Serializer.scala:171)\n",
       "\tat org.apache.spark.serializer.DeserializationStream$$anon$2.getNext(Serializer.scala:201)\n",
       "\tat org.apache.spark.serializer.DeserializationStream$$anon$2.getNext(Serializer.scala:198)\n",
       "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)\n",
       "\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)\n",
       "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n",
       "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)\n",
       "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)\n",
       "\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n",
       "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
       "\tat org.apache.spark.graphx.impl.ShippableVertexPartition$.apply(ShippableVertexPartition.scala:60)\n",
       "\tat org.apache.spark.graphx.VertexRDD$$anonfun$2.apply(VertexRDD.scala:328)\n",
       "\tat org.apache.spark.graphx.VertexRDD$$anonfun$2.apply(VertexRDD.scala:325)\n",
       "\tat org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:99)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)\n",
       "\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:262)\n",
       "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:264)\n",
       "\tat org.apache.spark.graphx.VertexRDD.compute(VertexRDD.scala:71)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:264)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:88)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n",
       "\tat java.lang.Thread.run(Thread.java:745)\n",
       "\n",
       "Driver stacktrace:\n",
       "StackTrace: org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283)\n",
       "org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271)\n",
       "org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270)\n",
       "scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
       "scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n",
       "org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270)\n",
       "org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)\n",
       "org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)\n",
       "scala.Option.foreach(Option.scala:236)\n",
       "org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697)\n",
       "org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496)\n",
       "org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458)\n",
       "org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447)\n",
       "org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
       "org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567)\n",
       "org.apache.spark.SparkContext.runJob(SparkContext.scala:1822)\n",
       "org.apache.spark.SparkContext.runJob(SparkContext.scala:1835)\n",
       "org.apache.spark.SparkContext.runJob(SparkContext.scala:1848)\n",
       "org.apache.spark.SparkContext.runJob(SparkContext.scala:1919)\n",
       "org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:905)\n",
       "org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)\n",
       "org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)\n",
       "org.apache.spark.rdd.RDD.withScope(RDD.scala:306)\n",
       "org.apache.spark.rdd.RDD.collect(RDD.scala:904)\n",
       "$line116.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:48)\n",
       "$line116.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:53)\n",
       "$line116.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:55)\n",
       "$line116.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:57)\n",
       "$line116.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:59)\n",
       "$line116.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:61)\n",
       "$line116.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:63)\n",
       "$line116.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:65)\n",
       "$line116.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:67)\n",
       "$line116.$read$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:69)\n",
       "$line116.$read$$iwC$$iwC$$iwC$$iwC.<init>(<console>:71)\n",
       "$line116.$read$$iwC$$iwC$$iwC.<init>(<console>:73)\n",
       "$line116.$read$$iwC$$iwC.<init>(<console>:75)\n",
       "$line116.$read$$iwC.<init>(<console>:77)\n",
       "$line116.$read.<init>(<console>:79)\n",
       "$line116.$read$.<init>(<console>:83)\n",
       "$line116.$read$.<clinit>(<console>)\n",
       "$line116.$eval$.<init>(<console>:7)\n",
       "$line116.$eval$.<clinit>(<console>)\n",
       "$line116.$eval.$print(<console>)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n",
       "sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "java.lang.reflect.Method.invoke(Method.java:606)\n",
       "org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n",
       "org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1340)\n",
       "org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n",
       "org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n",
       "org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n",
       "com.ibm.spark.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreter.scala:354)\n",
       "com.ibm.spark.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreter.scala:349)\n",
       "com.ibm.spark.global.StreamState$.withStreams(StreamState.scala:80)\n",
       "com.ibm.spark.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1.apply(ScalaInterpreter.scala:348)\n",
       "com.ibm.spark.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1.apply(ScalaInterpreter.scala:348)\n",
       "com.ibm.spark.utils.TaskManager$$anonfun$add$2$$anon$1.run(TaskManager.scala:123)\n",
       "java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n",
       "java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n",
       "java.lang.Thread.run(Thread.java:745)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foodNetwork.vertices.filter(_._1 == 10292).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "res: Array[(org.apache.spark.graphx.VertexId, FNNode)] =\n",
    "Array((10292,Compound(1-octanol,111-87-5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가장 많은 종류의 성분을 가지고 있는 것은 the black tea 그리고 가장 자주 함유되는 성분은 1-octanol이었다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 이 다음 코드부터는 정상 구동 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Building a weighted social ego network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• ego.edges: These are directed edges in the ego network. The ego node does not appear in this list, but it is assumed that it follows every node ID that appears in the file.  \n",
    "• ego.feat : This features for each of the nodes that appear in the edge file.  \n",
    "• ego.featnames: This is the name of each of the feature dimensions. The\n",
    "feature is 1 if the user has this property in their profile, and 0 otherwise.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.graphx._\n",
    "import org.apache.spark.rdd._\n",
    "import breeze.linalg.SparseVector\n",
    "import scala.io.Source\n",
    "import scala.math.abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type Feature = breeze.linalg.SparseVector[Int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val featureMap: Map[Long, Feature] =\n",
    "     Source.fromFile(\"./data/ego.feat\").\n",
    "        getLines().\n",
    "        map{line =>\n",
    "        val row = line split ' '\n",
    "        val key = abs(row.head.hashCode.toLong)\n",
    "        val feat = SparseVector(row.tail.map(_.toInt))\n",
    "        (key, feat)\n",
    "        }.toMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "val key = abs(row.head.hashCode.toLong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val edges: RDD[Edge[Int]] =\n",
    "     sc.textFile(\"./data/ego.edges\").\n",
    "        map {line =>\n",
    "           val row = line split ' '\n",
    "           val srcId = abs(row(0).hashCode.toLong)\n",
    "           val dstId = abs(row(1).hashCode.toLong)\n",
    "           val srcFeat = featureMap(srcId)\n",
    "           val dstFeat = featureMap(dstId)\n",
    "           val numCommonFeats = srcFeat dot dstFeat\n",
    "           Edge(srcId, dstId, numCommonFeats)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val egoNetwork: Graph[Int,Int] = Graph.fromEdges(edges, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1852"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "egoNetwork.edges.filter(_.attr == 3).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9353"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "egoNetwork.edges.filter(_.attr == 2).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107934"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "egoNetwork.edges.filter(_.attr == 1).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the degrees of the network nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Degree histogram of the social ego networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can compute the degrees of the connections in the ego network. Let's look at the maximum and minimum degrees in the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1643293729,1084)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "egoNetwork.degrees.reduce(max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Compile Error\n",
       "Message: <console>:50: error: not found: value min\n",
       "              egoNetwork.degrees.reduce(min)\n",
       "                                        ^\n",
       "StackTrace: "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "egoNetwork.degrees.reduce(min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2: max는 되는데 왜 min은 에러가날까요..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(550756674,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array((1,15), (2,19), (3,12), (4,17), (5,11), (6,19), (7,14), (8,9), (9,8), (10,10), (11,1), (12,9), (13,6), (14,7), (15,8), (16,6), (17,5), (18,5), (19,7), (20,6), (21,8), (22,5), (23,8), (24,1), (25,2), (26,5), (27,8), (28,4), (29,6), (30,7), (31,5), (32,10), (33,6), (34,10), (35,5), (36,9), (37,7), (38,8), (39,5), (40,4), (41,3), (42,1), (43,3), (44,5), (45,7), (46,6), (47,3), (48,6), (49,1), (50,9), (51,5), (52,8), (53,8), (54,4), (55,2), (56,5), (57,7), (58,4), (59,8), (60,9), (61,12), (62,5), (63,15), (64,5), (65,7), (66,6), (67,9), (68,4), (69,5), (70,4), (71,7), (72,9), (73,10), (74,2), (75,6), (76,7), (77,10), (78,7), (79,9), (80,5), (81,3), (82,4), (83,7), (84,7), (85,4), (86,6), (87,6), (88,10), (89,4), (90,6), (91,3), (92,4), (93,7), (94,4), (95,6)..."
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "egoNetwork.degrees.\n",
    "     map(t => (t._2,t._1)).\n",
    "     groupByKey.map(t => (t._1,t._2.size)).\n",
    "     sortBy(_._1).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번 챕터에서는 스파크에서 graph를 만드는 여러가지 방법을   \n",
    "online social networks, food science, and e-mail communications 세가지 real dataset을 이용해서 공부했습니다.   \n",
    "그래프를 구성하기 위해서는  data preparation and wrangling을 위한 노력이 필요하지만, \n",
    "GraphX는 우리가 만들고자하는 graph 표현과 데이터셋의 형태에 따라 여러가지 graph builder 함수를 제공하며 이는 다른 graph-processing frameworks과의 차별된 기능성을 제공합니다. 또한, 기본 통계량, 그래프의 특성을 보았으며 이는 구조의 특성을 파악하고 표현을 이해하는데에 유용합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음 챕터에선, 그래프 분석에 대해 더 깊이 들어가, 데이터 시각화 툴을 사용하고 새로운 그래프 이론 개념과 connectedness, triangle counting, PageRank과 같은 알고리즘들을 다룹니다!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.10.4",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
