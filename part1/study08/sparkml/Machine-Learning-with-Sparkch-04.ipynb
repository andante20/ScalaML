{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch 4. Building a Recommendation Engine with Spark\n",
    "\n",
    "* The idea behind recommendation engines is to predict what people might like and to uncover relationships between items to aid in the discovery process\n",
    "\n",
    "* we could use a recommendation engine to show our users movies that they might enjoy. If we can do this well, we could keep our users engaged using our service, which is good for both our users and us\n",
    "\n",
    "## Types of recommendation models\n",
    "* Recommendation engines are most effective in two general scenarios(which are not mutally exclusive)\n",
    "\n",
    "* Large number of available options for users\n",
    "* A significant degree of personal taste involved\n",
    "\n",
    "### Content-based filtering\n",
    "* 아이템 속성 기반(제목, 태그 등), 오디오 비디오 컨텐츠에서 추출한 속성도 포함 가능\n",
    "* 사용자 프로파일 사용자 속성을 기반 유사한 사용자 매칭\n",
    "* 사용자와 관련된 아이템의 속성의 조합으로 사용자를 표현\n",
    "* 이것이 사용자 프로파일\n",
    "\n",
    "### Collaborative filtering\n",
    "* 다수의 사람들의 선호도를 기반\n",
    "* 사용자 기반 추천, 아이템 기반 추천 \n",
    "* 사용자, 아이템 기반 추천은 일반적으로 nearest-neighbor model에 속함\n",
    "\n",
    "\n",
    "## Matrix Factorization\n",
    "* spark의 추천 모델은 현재 matrix factorization\n",
    "* 이런 종류의 모델은 꾸준히 협업 필터링에서 좋은 성능을 냄\n",
    "\n",
    "### Explicit matrix factorizaton\n",
    "* rations, thumbs up, likes 와 같은 명시적 선호도 자료에 기반\n",
    "* 사용자와 아이템의 2차원 행렬로 표시(very sparse matrix)\n",
    "***\n",
    "```\n",
    "Tom, Star Wars, 5\n",
    "Jane, Titanic, 4\n",
    "Bill, Batman, 3\n",
    "Jane, Star Wars, 2\n",
    "Bill, Titanic, 3\n",
    "```\n",
    "***\n",
    "<img src=image4notebook/4_1.png width=500 height=250 />\n",
    "* matrix factorization, matrix completion\n",
    "<img src=image4notebook/4_2.png width=500 height=500 />\n",
    "* two matrices : U x k, I x k\n",
    "* 원본 평가 행렬은 very sparse, 각 요인 행렬은 dense\n",
    "<img src=image4notebook/4_3.png width=500 height=500 />\n",
    "* latent feature models\n",
    "* 사용자와 아이템에 대한 평가를 예측하기 위해서, 관련된 user-factor matrix 행과, item-factor matrix 행의 벡터 내적(vector dot product)을 계산(아래 이미지 참조)\n",
    "<img src=image4notebook/4_4.png width=500 height=500 />\n",
    "* 두 아이템 사이의 유사도를 알기위해 nearest-neighber model 사용(item-factor 벡터 사이 유사도 제외)\n",
    "<img src=image4notebook/4_5.png width=500 height=500 />\n",
    "* factorization 모델의 좋은 점은 모델 생성 후 추천 계산의 용이성이 높음\n",
    "* 사용자와 아이템 집합이 큰 경우 저장, 계산이 어려움\n",
    "* factorization 모델의 단점은 NNM모델에 비해 해석 및 이해가 어려움\n",
    "* 모델 훈련 단계에서 많은 연산이 필요함\n",
    "\n",
    "### implicit matrix factorization\n",
    "* 직접적인 평가가 아닌 암묵적 피드백\n",
    "* 영화를 보거나 구매하는 행위, 영화 시청 횟수 등\n",
    "* P : binary preference matrix, C : confidence weights matrix\n",
    "* P : 사용자가 본 영화, C : 사용자가 영화를 본 횟수\n",
    "* 벡터 내적을 통해 추천을 계산 시 스코아는 평가를 나타내기보다 사용자의 선호도를 나타냄\n",
    "<img src=image4notebook/4_6.png width=500 height=200 />\n",
    "\n",
    "### Alternating least squares\n",
    "* ALS는 matrix factorization 문제를 해결하는 최적화 기술\n",
    "* 좋은 성능, 병렬처리 구현이 상대적으로 용이\n",
    "* 1.0.0 ~ 1.4.1 현재 버전까지도 Collaborative filtering 알고리즘은ALS만을 제공하고 있음\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting the right features from your data\n",
    "\n",
    "### Extrating features from the Movie-Lens 100k dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "196\t242\t3\t881250949"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rawData = sc.textFile(\"/Users/Limsangbae/ml-100k/u.data\")\n",
    "rawData.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(196, 242, 3)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rawRatings = rawData.map(_.split(\"\\t\").take(3))\n",
    "rawRatings.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.recommendation.ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Compile Error\n",
       "Message: <console>:24: error: ambiguous reference to overloaded definition,\n",
       "both method train in object ALS of type (ratings: org.apache.spark.rdd.RDD[org.apache.spark.mllib.recommendation.Rating], rank: Int, iterations: Int)org.apache.spark.mllib.recommendation.MatrixFactorizationModel\n",
       "and  method train in object ALS of type (ratings: org.apache.spark.rdd.RDD[org.apache.spark.mllib.recommendation.Rating], rank: Int, iterations: Int, lambda: Double)org.apache.spark.mllib.recommendation.MatrixFactorizationModel\n",
       "match expected type ?\n",
       "              ALS.train\n",
       "                  ^\n",
       "StackTrace: "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ALS.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Compile Error\n",
       "Message: <console>:26: error: not enough arguments for method apply: (user: Int, product: Int, rating: Double)org.apache.spark.mllib.recommendation.Rating in object Rating.\n",
       "Unspecified value parameters user, product, rating.\n",
       "              Rating()\n",
       "                    ^\n",
       "StackTrace: "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.mllib.recommendation.Rating\n",
    "Rating()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val ratings = rawRatings.map { \n",
    "    case Array(user,movie, rating) => Rating(user.toInt, movie.toInt, rating.toDouble)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Rating(196,242,3.0)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the recommendation model\n",
    "\n",
    "### Training a model on the MovieLens 100k  dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 모델을 훈련 시킬 준비가 되었음!\n",
    "    * rank : ALS 모델의 factor의 수, factor는 low-rank approximation matrices의 hidden feature의 수. 일반적으로 큰 수가 좋으나 메모리 사용(계산 및 저장)에 직접적 영향이 있음. Trade- off. 10~200 정도가 일반적으로 합리적 범위\n",
    "    * iterations : 반복 수행 횟수. ALS  알고리즘은 상대적으로 적은 반복만으로도 좋은 성능. 10 정도가 일반적으로 좋음\n",
    "    * lambda : 모델의 균일화(regularization) 제어, over fitting을 제어. 데이터 특성에 맞게 조절이 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val model= ALS.train(ratings, 50,10, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "943"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.userFeatures.count // lazy transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1682"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.productFeatures.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1682"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.productFeatures.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Limsangbae  Macintosh  ~  ml-100k  $  wc -l u.user\n",
    "     943 u.user\n",
    " Limsangbae  Macintosh  ~  ml-100k  $  wc -l u.item\n",
    "    1682 u.item\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the recommendation model\n",
    "\n",
    "### User recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val predictedRating = model.predict(789,123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.911786142405169"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictedRating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rating(789,195,5.697909233600918)\n",
      "Rating(789,156,5.640288372299024)\n",
      "Rating(789,177,5.549697775269709)\n",
      "Rating(789,96,5.458112117882252)\n",
      "Rating(789,223,5.446679308973188)\n",
      "Rating(789,433,5.393072357021246)\n",
      "Rating(789,192,5.374565041036448)\n",
      "Rating(789,429,5.360461561295375)\n",
      "Rating(789,180,5.353779019521968)\n",
      "Rating(789,653,5.346538771637403)\n",
      "[Lorg.apache.spark.mllib.recommendation.Rating;@4c20df2a\n"
     ]
    }
   ],
   "source": [
    "val userId=789\n",
    "val K=10\n",
    "val topKRecs = model.recommendProducts(userId,K)\n",
    "println(topKRecs.mkString(\"\\n\"))\n",
    "println(topKRecs.toString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cat u.data | grep \"^789\"| sort -n -k2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspecting the recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Frighteners, The (1996)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val movies = sc.textFile(\"/Users/Limsangbae/ml-100k/u.item\")\n",
    "val titles = movies.map(line => line.split(\"\\\\|\").take(2)).map(array=>(array(0).toInt,array(1))).collectAsMap()\n",
    "titles(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map(137 -> Big Night (1996), 891 -> Bent (1997), 550 -> Die Hard: With a Vengeance (1995), 1205 -> Secret Agent, The (1996), 146 -> Unhook the Stars (1996), 864 -> My Fellow Americans (1996), 559 -> Interview with the Vampire (1994), 218 -> Cape Fear (1991), 568 -> Speed (1994), 227 -> Star Trek VI: The Undiscovered Country (1991), 765 -> Boomerang (1992), 1115 -> Twelfth Night (1996), 774 -> Prophecy, The (1995), 433 -> Heathers (1989), 92 -> True Romance (1993), 1528 -> Nowhere (1997), 846 -> To Gillian on Her 37th Birthday (1996), 1187 -> Switchblade Sisters (1975), 1501 -> Prisoner of the Mountains (Kavkazsky Plennik) (1996), 442 -> Amityville Curse, The (1990), 1160 -> Love! Valour! Compassion! (1997), 101 -> Heavy Metal (1981), 1196 -> Savage Nights (Nuits fauves, ..."
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles.toString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    }
   ],
   "source": [
    "val moviesForUser=ratings.keyBy(_.user).lookup(789)\n",
    "println(moviesForUser.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Syntax Error.\n",
       "Message: \n",
       "StackTrace: "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Godfather, The (1972),5.0)\n",
      "(Trainspotting (1996),5.0)\n",
      "(Dead Man Walking (1995),5.0)\n",
      "(Star Wars (1977),5.0)\n",
      "(Swingers (1996),5.0)\n",
      "(Leaving Las Vegas (1995),5.0)\n",
      "(Bound (1996),5.0)\n",
      "(Fargo (1996),5.0)\n",
      "(Last Supper, The (1995),5.0)\n",
      "(Private Parts (1997),4.0)\n"
     ]
    }
   ],
   "source": [
    "moviesForUser.sortBy(-_.rating).take(10).map(rating=>(titles(rating.product),rating.rating)).foreach(println)\n",
    "// soryBy \" - \"descending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Terminator, The (1984),5.697909233600918)\n",
      "(Reservoir Dogs (1992),5.640288372299024)\n",
      "(Good, The Bad and The Ugly, The (1966),5.549697775269709)\n",
      "(Terminator 2: Judgment Day (1991),5.458112117882252)\n",
      "(Sling Blade (1996),5.446679308973188)\n",
      "(Heathers (1989),5.393072357021246)\n",
      "(Raging Bull (1980),5.374565041036448)\n",
      "(Day the Earth Stood Still, The (1951),5.360461561295375)\n",
      "(Apocalypse Now (1979),5.353779019521968)\n",
      "(Touch of Evil (1958),5.346538771637403)\n"
     ]
    }
   ],
   "source": [
    "topKRecs.map(rating=>(titles(rating.product),rating.rating)).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 이 항목과 가장 비슷한 항목은 어떤것인가?\n",
    "* 유사도 측정 방법\n",
    "    * Pearson correlation\n",
    "    * cosine similarity for real-valued vectors\n",
    "    * jaccard similarity for binary vectors\n",
    "    \n",
    "### Generating similar movies for the MovieLens 100k dataset    \n",
    "* 현재 MatrixFactorizationModel API는 item2item 유사도 계산을 직접적으로 지원하지 않음\n",
    "* Cosine similarity 사용(-1 ~ 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.jblas.DoubleMatrix\n",
    "val aMatrix = new DoubleMatrix(Array(1,0,2,0,3.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.000000; 0.000000; 2.000000; 0.000000; 3.000000]\n"
     ]
    }
   ],
   "source": [
    "println(aMatrix.toString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cosineSimilarity(vec1: DoubleMatrix, vec2: DoubleMatrix) : \n",
    "Double = { vec1.dot(vec2)/(vec1.norm2()*vec2.norm2())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// 동일한 벡터 사이의 코사인 유사도는 1\n",
    "val itemId = 567\n",
    "val itemFactor = model.productFeatures.lookup(itemId).head\n",
    "val itemVector = new DoubleMatrix(itemFactor)\n",
    "cosineSimilarity(itemVector, itemVector) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val sims = model.productFeatures.map{ case (id,factor)=>\n",
    "val factorVector = new DoubleMatrix(factor)\n",
    "val sim = cosineSimilarity(factorVector, itemVector)\n",
    "(id,sim)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Syntax Error.\n",
       "Message: \n",
       "StackTrace: "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// K는 10으로 \n",
    "val sortedSims = sims.top(K) (Ordering.by[(Int,Double), Double] { case (id, similarity) => similarity})\n",
    "//top() spark 함수를 이용하여 분산 방식으로 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(567,1.0)\n",
      "(16,0.7021405671989809)\n",
      "(433,0.6965832620396708)\n",
      "(109,0.6871238986088308)\n",
      "(403,0.6805367352655004)\n",
      "(741,0.6791489354428673)\n",
      "(1007,0.6766634661747077)\n",
      "(248,0.6721551340936744)\n",
      "(219,0.6718872224527191)\n",
      "(413,0.668481869082473)\n"
     ]
    }
   ],
   "source": [
    "println(sortedSims.take(10).mkString(\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wes Craven's New Nightmare (1994)\n"
     ]
    }
   ],
   "source": [
    "println(titles(itemId))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val sortedSims2 = sims.top(K + 1)(Ordering.by[(Int, Double), Double] { case (id, similarity) => similarity })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(French Twist (Gazon maudit) (1995),0.7021405671989809)\n",
      "(Heathers (1989),0.6965832620396708)\n",
      "(Mystery Science Theater 3000: The Movie (1996),0.6871238986088308)\n",
      "(Batman (1989),0.6805367352655004)\n",
      "(Last Supper, The (1995),0.6791489354428673)\n",
      "(Waiting for Guffman (1996),0.6766634661747077)\n",
      "(Grosse Pointe Blank (1997),0.6721551340936744)\n",
      "(Nightmare on Elm Street, A (1984),0.6718872224527191)\n",
      "(Tales from the Crypt Presents: Bordello of Blood (1996),0.668481869082473)\n",
      "(Some Kind of Wonderful (1987),0.6677884697880286)\n"
     ]
    }
   ],
   "source": [
    "//println이 책에는 빠져있음\n",
    "println(sortedSims2.slice(1, 11).map{ case (id, sim) => (titles(id), sim) }.mkString(\"\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the performance of recommendation models\n",
    "* 훈련된 모델이 좋은 모델이라는 것을 어떻게 알 수 있을까?\n",
    "* 예측 성능 평가 필요함\n",
    "* 두가지 일반적인 측정 지표(Mean Squared Error, Mean average precision at K)\n",
    "\n",
    "### Mean Squared Error(MSE)\n",
    "* 오차 제곱의 합을 관측값(행)으로 나눈 것\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rating(789,1012,4.0)\n"
     ]
    }
   ],
   "source": [
    "val actualRating = moviesForUser.take(1)(0)\n",
    "println(actualRating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.010584985135074\n"
     ]
    }
   ],
   "source": [
    "val predictRating = model.predict(789, actualRating.product)\n",
    "//println(actualRating.product)\n",
    "//val predictRating = model.predict(789, 1024)\n",
    "println(predictRating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.007781684671761088\n"
     ]
    }
   ],
   "source": [
    "val squareError = math.pow(predictedRating - actualRating.rating, 2.0)\n",
    "println(squareError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val usersProducts = ratings.map{ case Rating(user, product, rating) => (user, product) }\n",
    "val predictions =model.predict(usersProducts).map{\n",
    "case Rating(user,product,rating) => ((user,product),rating) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((92,386),(3.0,2.4415577029127324))\n"
     ]
    }
   ],
   "source": [
    "val ratingsAndPredictions = ratings.map{\n",
    "case Rating(user, product, rating) => ((user,product), rating) }.join(predictions)\n",
    "println(ratingsAndPredictions.take(1).mkString(\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 0.0850860348619457\n"
     ]
    }
   ],
   "source": [
    "val MSE = ratingsAndPredictions.map{\n",
    "case((user,product),(actual,predicted)) =>\n",
    "math.pow((actual-predicted),2)\n",
    "}.reduce(_+_) / ratingsAndPredictions.count\n",
    "println(\"MSE = \" + MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE = 0.2916951059958766\n"
     ]
    }
   ],
   "source": [
    "val RMSE = math.sqrt(MSE)\n",
    "println(\"RMSE = \" + RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean average precision at K\n",
    "* Mean average precision at K(MAPK) \n",
    "* APK는 정보탐색에 일반적으로 사용되는 지표\n",
    "* precision = Relevant Retrieved/Retrieved\n",
    "* Average Precision = 관련된 문서 찾은 것의 precision을 전체 관련된 문서의 수로 나눈 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def avgPrecisionK(actual: Seq[Int], predicted :\n",
    "Seq[Int], k:Int): Double={\n",
    "val predK = predicted.take(k)\n",
    "var score =0.0\n",
    "var numHits =0.9\n",
    "for ((p, i) <-predK.zipWithIndex) {\n",
    "    if (actual.contains(p)) {\n",
    "        numHits += 1.0\n",
    "        score += numHits/(i.toDouble + 1.0)\n",
    "        }\n",
    "    }\n",
    "    if (actual.isEmpty) {\n",
    "        1.0\n",
    "    } else {\n",
    "        score /scala.math.min(actual.size,k).toDouble\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ArrayBuffer(1012, 127, 475, 93, 1161, 286, 293, 9, 50, 294, 181, 1, 1008, 508, 284, 1017, 137, 111, 742, 248, 249, 1007, 591, 150, 276, 151, 129, 100, 741, 288, 762, 628, 124)\n"
     ]
    }
   ],
   "source": [
    "val actualMovies = moviesForUser.map(_.product)\n",
    "println(actualMovies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195\n",
      "156\n",
      "177\n",
      "96\n",
      "223\n",
      "433\n",
      "192\n",
      "429\n",
      "180\n",
      "653\n"
     ]
    }
   ],
   "source": [
    "val predictedMovies = topKRecs.map(_.product)\n",
    "println(predictedMovies.mkString(\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "val apk10 = avgPrecisionK(actualMovies, predictedMovies, 10)\n",
    "println(apk10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "In order to compute the APK for each user and average them to compute the overall MAPK, we will need to generate the list of recommendations for each user in our dataset. While this can be fairly intensive on a large scale, we can distribute the computation using our Spark functionality. However, one limitation is that each worker must have the full item-factor matrix available so that it can compute the dot product between the relevant user vector and all item vectors. This can be a problem when the number of items is extremely high as the item matrix must fit in the memory of one machine.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1682,50)\n"
     ]
    }
   ],
   "source": [
    "val itemFactors = model.productFeatures.map { case (id, factor) => factor }.collect()\n",
    "val itemMatrix = new DoubleMatrix(itemFactors)\n",
    "println(itemMatrix.rows, itemMatrix.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val imBroadcast = sc.broadcast(itemMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MapPartitionsRDD[465] at map at <console>:44\n"
     ]
    }
   ],
   "source": [
    "val allRecs = model.userFeatures.map{ case (userId, array) => \n",
    "  val userVector = new DoubleMatrix(array)\n",
    "  val scores = imBroadcast.value.mmul(userVector)\n",
    "  val sortedWithId = scores.data.zipWithIndex.sortBy(-_._1)\n",
    "  val recommendedIds = sortedWithId.map(_._2 + 1).toSeq\n",
    "  (userId, recommendedIds)\n",
    "}\n",
    "\n",
    "println(allRecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val userMovies = ratings.map{ case Rating(user, product, rating) => (user, product) }.groupBy(_._1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Average Precision at K = 0.05180138952010632\n"
     ]
    }
   ],
   "source": [
    "val K = 10\n",
    "val MAPK = allRecs.join(userMovies).map{ case (userId, (predicted, actualWithIds)) => \n",
    "  val actual = actualWithIds.map(_._2).toSeq\n",
    "  avgPrecisionK(actual, predicted, K)\n",
    "  }.reduce(_ + _) / allRecs.count\n",
    "println(\"Mean Average Precision at K = \" + MAPK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using MLlib's built-in evaluation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* MSE, RMSE, MAPK\n",
    "* 실은 계산할 필요 없음. 다 제공함 T_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error = 0.08508603486194566\n",
      "Root Mean Squared Error = 0.29169510599587656\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.mllib.evaluation.RegressionMetrics\n",
    "val predictedAndTrue = ratingsAndPredictions.map { \n",
    "case ((user, product), (predicted, actual)) => (predicted, actual) }\n",
    "val regressionMetrics = new RegressionMetrics(predictedAndTrue)\n",
    "\n",
    "println(\"Mean Squared Error = \" + regressionMetrics.meanSquaredError)\n",
    "println(\"Root Mean Squared Error = \" + regressionMetrics.rootMeanSquaredError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Average Precision = 0.0725445180994263\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.mllib.evaluation.RankingMetrics\n",
    "val predictedAndTrueForRanking = allRecs.join(userMovies).map{ case (userId, (predicted, actualWithIds)) => \n",
    "  val actual = actualWithIds.map(_._2)\n",
    "  (predicted.toArray, actual.toArray)\n",
    "}\n",
    "val rankingMetrics = new RankingMetrics(predictedAndTrueForRanking)\n",
    "println(\"Mean Average Precision = \" + rankingMetrics.meanAveragePrecision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Average Precision = 0.07768046214275937\n"
     ]
    }
   ],
   "source": [
    "val MAPK2000 = allRecs.join(userMovies).map{ case (userId, (predicted, actualWithIds)) => \n",
    "  val actual = actualWithIds.map(_._2).toSeq\n",
    "  avgPrecisionK(actual, predicted, 2000)\n",
    "}.reduce(_ + _) / allRecs.count\n",
    "println(\"Mean Average Precision = \" + MAPK2000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark 1.4.0 (Scala 2.11.4)",
   "language": "scala",
   "name": "spark"
  },
  "language_info": {
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
