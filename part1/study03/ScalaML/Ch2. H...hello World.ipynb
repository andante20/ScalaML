{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prologue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Keywords covered__ \n",
    "* Applying the concept of monadic design\n",
    "* Scala's advanced functional features: e.g. dependency injection\n",
    "* The bias-variance trade-off\t\n",
    "* Overfitting\t\t\t\t\t \t\t\t\n",
    "* Training, test, and validation sets\t\n",
    "* Precision, recall, and F score \t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A model is composed of features (also known as attributes or variables), and a set of relation between those features:\n",
    "\t\t\t\t\tf(x, y) = x.sin(2y)\n",
    "\t\t \t \t \tf(x, y) < 20 \t\t\t\t\n",
    "* a monoid for which the set is a group of observations and the operator is the function implementing the model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[Shapes and forms]__\t \t\t\t\t\t\t\t\n",
    "* Parametric: This consists of functions and equations \n",
    "(for example, y = sin(2t + w))\t\n",
    "* Differential: This consists of ordinary and partial differential equations\n",
    "(for example, dy = 2x.dx)\t\n",
    "* Probabilistic: This consists of probability distributions \n",
    "(for example, p (x|c) = exp (k.logx – x)/x!)\n",
    "* Graphical: This consists of graphs that abstract out the conditional independence between variables \n",
    "(for example, p(x,y|c) = p(x|c).p(y|c))\n",
    "* Directed graphs: This consists of temporal and spatial relationships \n",
    "(for example, a scheduler)\n",
    "* Numerical method: This consists of finite elements and methods such as Newton-Raphson\t\n",
    "* Chemistry: This consists of formula and components (for example, H2O, Fe + C12 = FeC13, and so on) \t\n",
    "* Taxonomy: This consists of a semantic definition and relationship of concepts \n",
    "(for example, APG/Eudicots/Rosids/Huaceae/Malvales)\n",
    "* Grammar and lexicon: This consists of a syntactic representation of documents \n",
    "(for example, Scala programming language)\n",
    "* Inference logic: This consists of a distribution pattern such as IF (stock vol > 1.5 * average) AND rsi > 80 THEN... \t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Model versus design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Modeling: Describing something you know. A model makes the assumption, which becomes an assertion if proven correct \t\n",
    "* Designing: This is manipulating representation for things you don't know. Designing can be seen as the exploration phase of modeling \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Selecting a model's features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Searching for new feature subsets.\n",
    "2. Evaluating these feature subsets using a scoring mechanism. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Extracting features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Purpose: to reduce the number of variables or dimensions of the model by eliminating redundant or irrelevant features.\n",
    "* by transforming the original set of observations into a smaller set at the risk of losing some vital information embedded in the original set. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Designing a workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Screen Shot 2015-06-27 at 4.46.48 AM.png\">\n",
    "* Scala's higher-order functions are particularly suitable for implementing configurable data transformations. \t\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#The computational framework "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Screen Shot 2015-06-27 at 4.50.37 AM.png\">\n",
    "* to define a trait and a method that describes the transformation of data by a computation unit (element of the workflow). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#The pipe operator "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* to define a symbolic representation of the transformation of different types of data without exposing the internal state of the algorithm implementing the data transformation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trait PipeOperator[-T, +U] {\n",
    "def |> (data: T): Option[U]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The |> operator transforms a data of the type T into a data of the type U and returns an option to handle internal errors and exceptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Monadic data transformation \n",
    "* to create a monadic design to implement the pipe operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class _FCT[+T](val _fct: T) {\n",
    "def map[U](c: T => U): _FCT[U] = new _FCT[U]( c(_fct))\n",
    "def flatMap[U](f: T =>_FCT[U]): _FCT[U] = f(_fct)\n",
    "def filter(p: T =>Boolean): _FCT[T] = if( p(_fct) ) new _FCT[T](_\n",
    "fct) else zeroFCT(_fct)\n",
    "def reduceLeft[U](f: (U,T) => U)(implicit c: T=> U): U = f(c(_fct),\n",
    "_fct)\n",
    "def foldLeft[U](zero: U)(f: (U, T) => U)(implicit c: T=> U): U =\n",
    "f(c(_fct), _fct)\n",
    "def foreach(p: T => Unit): Unit = p(_fct)\n",
    "} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The methods of the _FCT class represent a subset of the traditional Scala higher methods for collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Transform[-T, +U](val op: PipeOperator[T, U]) extends _ FCT[Function[T, Option[U]]](op.|>) {\n",
    "def |>(data: T): Option[U] = _fct(data) } "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* You can create any algorithm by just implementing the PipeOperator trait, after all.\n",
    "* The reason is that Transform has a richer protocol (methods) and enables developers to create a complex workflow as an alternative to the delimited continuation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Examlple: a generic function composition or data transformation composition using the monadic approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val op = new PipeOperator[Int, Double] {\n",
    "def |> (n: Int):Option[Double] =Some(Math.sin(n.toDouble)) }\n",
    "def g(f: Int =>Option[Double]): (Int=> Long) = {\n",
    "     (n: Int) => {\n",
    "       f(n) match {\n",
    "           case Some(x) => x.toLong\n",
    "         case None => -1L\n",
    "} }\n",
    "   }\n",
    "   val gof = new Transform[Int,Double](op).map(g(_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Dependency injection\n",
    "* workflow composed of configurable data transformations requires a dynamic modularization (substitution) of the different stages of the workflow. \n",
    "* __Cake pattern__: an advanced class composition pattern that uses mix-in traits to meet the demands of a configurable computation workflow (also known as stackable modification traits)\n",
    "* __Dependency injection__: a reverse look up and binding to dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* _for classification cases_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "  val myApp = new Classification with Validation with PreProcessing {\n",
    "   val filter = .. }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* _then at a later stage, need to an unsupervised clustering algorithm : code duplication and lack of flexibility_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "   val myApp = new Clustering with Validation with PreProcessing { val\n",
    "   filter = ..  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The solution is to use the self type in the definition of the newly composed PreProcessingWithValidation trait:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "trait PreProcessiongWithValidation extends PreProcessing { self: Validation =>\n",
    "        val filter = ..\n",
    "   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val myApp = new Classification with PreProcessingWithValidation { val validation: Validation\n",
    "￼￼￼￼￼￼￼￼￼￼￼￼}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Workflow modules\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The data transformation defined by the PipeOperator instance is dynamically injected into the module by initializing the abstract value.\n",
    "* three parameterized modules representing the preprocessing, processing, and post-processing stages of a workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trait PreprocModule[-T, +U] { val preProc: PipeOperator[T, U] }\n",
    "   trait ProcModule[-T, +U] { val proc: PipeOperator[T, U] }\n",
    "   trait PostprocModule[-T, +U] { val postProc: PipeOperator[T, U] }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* One characteristic of the Cake pattern is to enforce strict modularity by initializing the abstract values with the type encapsulated in the module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trait ProcModule[-T, +U] {\n",
    "      val proc: PipeOperator [T, U]\n",
    "      class Classification[-T, +U] extends PipeOperator [T,U] { }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#The workflow factory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* The next step is to write the different modules into a workflow\n",
    "* using the self reference to the stack of the three traits defined in the previous paragraph."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "  class WorkFlow[T, U, V, W] {\n",
    "     self: PreprocModule[T,U] with ProcModule[U,V] with\n",
    "   PostprocModule[V,W] =>\n",
    "       def |> (data: T): Option[W] = {\n",
    "          preProc |> data match {\n",
    "            case Some(input) => {\n",
    "             proc |> input match {\n",
    "               case Some(output) => postProc |> output\n",
    "               case None => { ...  }\n",
    "} }\n",
    "           case None => { ... }\n",
    "         }\n",
    "} }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* you can either create a workflow with a stack of two traits or initialize the third with the PipeOperator identity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def identity[T] = new PipeOperator[T,T] {\n",
    "      override def |> (data:T): Option[T] = Some(data)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Sampler(val samples: Int) extends PipeOperator[Double => Double, DblVector] {\n",
    "      override def |> (f: Double => Double): Option[DblVector] =\n",
    "      Some(Array.tabulate(samples)(n => f(n.toDouble/samples)) )\n",
    "   }\n",
    "class Normalizer extends PipeOperator[DblVector, DblVector] { override def |> (data: DblVector): Option[DblVector] =\n",
    "        Some(Stats[Double](data).normalize)\n",
    "   }\n",
    "class Reducer extends PipeOperator[DblVector, Int] { override def |> (data: DblVector): Option[Int] = Range(0, data.size) find(data(_) == 1.0)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"Screen Shot 2015-06-27 at 11.55.56 AM.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val dataflow = new Workflow[Double => Double, DblVector, DblVector, Int]\n",
    "                with PreprocModule[Double => Double, DblVector]\n",
    "                  with ProcModule[DblVector, DblVector]\n",
    "                    with PostprocModule[DblVector, Int] {\n",
    "     val preProc: PipeOperator[Double => Double,DblVector] = new\n",
    "   Sampler(100) //1\n",
    "   \n",
    "     val proc: PipeOperator[DblVector,DblVector]= new Normalizer //1\n",
    "     val postProc: PipeOperator[DblVector,Int] = new Reducer//1\n",
    "   }\n",
    "dataflow |> ((x: Double) => Math.log(x+1.0)+Random.nextDouble) match { case Some(index) => ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* Scala's strong type checking catches any inconsistent data types at compilation time.\n",
    "* It reduces the development cycle because runtime errors are more difficult to track down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Examples of workflow components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##The preprocessing module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* use the time series class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class XTSeries[T](label: String, arr: Array[T]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* The preprocessing algorithms such as moving average or discrete Fourier filters are encapsulated into a preprocessing module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "  trait PreprocessingModule[T] {\n",
    "     val preprocessor: Preprocessing[T]  //1\n",
    "abstract class Preprocessing[T] { //2 def execute(xt: XTSeries[T]): Unit\n",
    "}\n",
    "abstract class MovingAverage[T] extends Preprocessing[T] with PipeOperator[XTSeries[T], XTSeries[Double]] { //3\n",
    "override def execute(xt: XTSeries[T]): Unit = this |> xt match { case Some(filteredData) => ...\n",
    "       case None => ...\n",
    "       }\n",
    "}\n",
    "class SimpleMovingAverage[@specialized(Double) T <% Double](period: Int)(implicit num: Numeric[T]) extends MovingAverage[T] {\n",
    "override def |> (xt: XTSeries[T]): Option[XTSeries[Double]] =\n",
    "...\n",
    "}\n",
    "class DFTFir[T <% Double](g: Double=>Double) extends Preprocessing[T]\n",
    "extends PreProcessing[T] with PipeOperator[XTSeries[T], XTSeries[Double]] {\n",
    "override def execute(xt: XTSeries[T]): Unit = this |> xt match { case Some(filteredData) => ...\n",
    "case None => ...\n",
    "}\n",
    "override def |> (xt: XTSeries[T]) : Option[XTSeries[Double]] }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##The clustering module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* the second example of a module for dependency-injection-based workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trait ClusteringModule[T] {\n",
    "     type EMOutput = List[(Double, DblVector, DblVector)]\n",
    "     val clustering: Clustering[T]\n",
    "     abstract class Clustering[T] {\n",
    "     def execute(xt: XTSeries[Array[T]]): Unit\n",
    "}\n",
    "     \n",
    "     class KMeans[T <% Double](K: Int, maxIters: Int, distance: (DblVector, Array[T]) => Double)(implicit order: Ordering[T], m: Manifest[T]) extends Clustering[T] with PipeOperator[XTSeries[Array [T]], List[Cluster[T]]] {\n",
    "override def |> (xt: XTSeries[Array[T]]): Option[List[Cluster[T]]]\n",
    "       override def execute(xt: XTSeries[Array[T]]): Unit = this |> xt\n",
    "   match {\n",
    "         case Some(clusters) => ...\n",
    "         case None => ...\n",
    "       }\n",
    "}\n",
    "class MultivariateEM[T <% Double](K: Int) extends Clustering[T] with PipeOperator[XTSeries[Array[T]], EMOutput] {\n",
    "override def |> (xt: XTSeries[Array[T]]): Option[EMOutput] = override def execute(xt: XTSeries[Array[T]]): Unit = this |> xt\n",
    "   match {\n",
    "          case Some(emOutput) => ...\n",
    "          case None => ...\n",
    "       }\n",
    "} }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Assessing a model\n",
    "* Evaluating a model is an essential part of the workflow. There is no point in creating the most sophisticated model if you do not have the tools to assess its quality. The validation process consists of defining some quantitative reliability criteria, setting\n",
    "a strategy such as an N-Fold cross-validation scheme, and selecting the appropriate labeled data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Validation\n",
    "\n",
    "* The purpose of this section is to create a Scala class to be used in future chapters for validating models. For starters, the validation process relies on a set of metrics to quantify the fitness of a model generated through training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Key metrics \n",
    "<img src=\"Screen Shot 2015-06-27 at 4.59.32 AM.png\">\n",
    "<img src=\"Screen Shot 2015-06-27 at 5.00.12 AM.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Implementation\n",
    "* validation formula using the same trait-based modular design used in creating the preprocessor and classifier modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " trait Validation {\n",
    "     def f1: Double\n",
    "     def precisionRecall: (Double, Double)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The array of actual versus expected class: actualExpected\n",
    "* The target class for true positive observations: tpClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class F1Validation(actualExpected: Array[(Int, Int)], tpClass: Int) extends Validation {\n",
    "         val counts = actualExpected.foldLeft(new Counter[Label])((cnt,\n",
    "       oSeries) => cnt + classify(oSeries._1, oSeries._2))\n",
    "         lazy val accuracy = {\n",
    "           val num = counts(TP) + counts(TN)\n",
    "           num.toDouble/counts.foldLeft(0)( (s,kv)  => s + kv._2)\n",
    "}\n",
    "lazy val precision = counts(TP).toDouble/(counts(TP) + counts(FP))\n",
    "lazy val recall = counts(TP).toDouble/(counts(TP) + counters(FN))\n",
    "override def f1: Double = 2.0*precision*recall/(precision + recall)\n",
    "override def precisionRecall: (Double, Double) = (precision, recall)\n",
    "         def classify(actual: Int, expected: Int): Label = {\n",
    "            if(actual == expected) { if(actual == tpClass) TP else TN }\n",
    "            else { if (actual == tpClass) FP else FN }\n",
    "￼} }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The precision and recall variables are defined as lazy so they are computed only once, when they are either accessed for the first time or the f1 and precisionRecall functions are invoked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "object Label extends Enumeration { type Label = Value\n",
    "val TP, TN, FP, FN = Value\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##K-fold cross-validation \n",
    "<img src=\"Screen Shot 2015-06-27 at 5.06.07 AM.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Bias-variance decomposition \n",
    "<img src=\"Screen Shot 2015-06-27 at 5.04.40 AM.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BiasVarianceEmulator[T <% Double](emul: Double => Double, nValues: Int) {\n",
    "def fit(fEst: List[Double => Double]): Option[XYTSeries] = { val rf = Range(0, fEst.size)\n",
    "val meanFEst = Array.tabulate(nValues)( x =>\n",
    "      rf.foldLeft(0.0)((s, n) => s+fEst(n)(x))/fEst.size) // 1\n",
    "        val r = Range(0, nValues)\n",
    "        Some(fEst.map(fe => {\n",
    "           r.foldLeft(0.0, 0.0)((s, x) => {\n",
    "             val diff = (fe(x) - meanFEst(x))/ fEst.size   // 2\n",
    "             (s._1 + diff*diff, s._2 + Math.abs(fe(x)-emul(x)))} )\n",
    "        }).toArray)\n",
    "     }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"formula.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val emul = (x: Double) => 0.2*x*(1.0 + Math.sin(x*0.05)) val fEst = List[(Double=>Double, String)] (\n",
    "     ((x: Double) => 0.2*x, \"y=x/5\"),\n",
    "     ((x: Double) => 0.0003*x*x + 0.18*x, \"y=3e-4.x^2-0.18x\"),\n",
    "     ((x: Double) =>0.2*x*(1+Math.sin(x*0.05),\n",
    "                   \"y=x(1+sin(x/20))/5\"))\n",
    "   val emulator = new BiasVarianceEmulator[Double](emul, 200)\n",
    "emulator.fit(fEst.map( _._1)) match { case Some(varBias) => show(varBias) case None => ...\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Overfitting \t\n",
    "Overfitting affects all aspects of the modeling process negatively, for example:\n",
    "\t\t\t\t\t\n",
    "* It is a sure sign of an overly complex model, which is difficult to debug and consumes computation resources\n",
    "* It makes the model representing minor fluctuations and noise\n",
    "* It may discover irrelevant relationships between observed and latent features\n",
    "* It has poor predictive performance \n",
    "\n",
    "there are well-proven solutions to reduce overfitting:\t\t\n",
    "* Increasing the size of the training set whenever possible\n",
    "* Reducing noise in labeled and input data through filtering\n",
    "* Decreasing the number of features using techniques such as principal components analysis\n",
    "* Modeling observable and latent noised using filtering techniques such as Kalman or autoregressive models\t\n",
    "* Reducing inductive bias in a training set by applying cross-validation\n",
    "* Penalizing extreme values for some of the model's features using regularization techniques "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.11",
   "language": "scala211",
   "name": "scala211"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
