{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch04. Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "분류 회귀 문제에서 라벨링은 위압적인 작업일 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "라벨링이 불가능 한 상황도 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "비지도 학습은 패턴과 유사성 검출에 사용된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정규성과 비정규성의 패턴을 발견하는 것이 비지도 학습의 목표다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 알고리즘을 다룬다.\n",
    "\n",
    "* K means: 관찰 피처 분류\n",
    "* 기대 최대(EM): 관찰과 잠복 리퍼 분류\n",
    "* 주 성분 분석(PCA): 모델의 차원 축소"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K means는 scala에서 구현되었고 EM과 PCA는 Common math에서 구현됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "많은 수의 데이터와 피처들의 문제는 갈수록 다루기 어렵게 되고 있다. \n",
    "\n",
    "많은 엔지니어링 분야가 큰 데어터 집합을 분류하는데 분할 정복을 사용하고 있다. 연속적이고 무한의 거대 데이터집합을 줄이는것이 목표가 되고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./figures/VisualizationOfDataClustering.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 기법은 벡터 양자화라고 불리고 관측 데이터를 작은 크기의 그룹으로 나누는 방법이다.이 방법의 혜택은 각 그룹의 대표성을 사용한 분석은 전체 데이터집합을 분석하는 것보다 간단하다는 것이다.\n",
    "\n",
    "벡터 양자화는 클러스터라는 그룹을 생성하기 위해 거리와 유사성 개념에 의존한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning vector quantization (LVQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector quantization는 vector quantization 학습과는 다르다. vector quantization은 승자독식 학습에 의존하는 인공 신경만의 특별한 경우이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 장은 대표적인 두개의 알고리즘을 소개한다.\n",
    "\n",
    "* K-means, which is used for quantitative types and minimizes the total error (known as the reconstruction error) given the number of clusters and the distance formula.\n",
    "* Expectation-maximization (EM), which is a two-step probabilistic approach that maximizes the likelihood estimates of a set of parameters. EM is particularly suitable to handle missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "K-means는 각 클러스터의 대표가 클로이드라고 불리는 클러스터의 중심으로 계산되는 대중적인 반복적 군집 알고리즘이다. 군집내에 거리를 기준으로 계산한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "관측치 간에 유사성을 측정하는 방법은 많으나 가장 적절한 측정은 직관적이어야 하고 계산 복잡도를 피해야 한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "세가지 유사성 측정을 고려해 보면\n",
    "\n",
    "1. The Manhattan distance\n",
    "2. The Euclidean distance\n",
    "3. Cosine of value observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Manhattan distance는 변수간의 절대 거리로 계산 {x i } and {y i }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./figures/manhattan.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction \u001b[36mmanhattan\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def manhattan[T <% Double, U <% Double]\n",
    "    (x: Array[T], y: Array[U]): Double = \n",
    "        (x,y).zipped.foldLeft(0.0)((s, t) => s + Math.abs(t._1 - t._2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres0\u001b[0m: \u001b[32mList\u001b[0m[(\u001b[32mInt\u001b[0m, \u001b[32mInt\u001b[0m)] = \u001b[33mList\u001b[0m(\n",
       "  \u001b[33m\u001b[0m(\u001b[32m1\u001b[0m, \u001b[32m3\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m2\u001b[0m, \u001b[32m4\u001b[0m),\n",
       "  \u001b[33m\u001b[0m(\u001b[32m3\u001b[0m, \u001b[32m5\u001b[0m)\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "List(1,2,3).zip(List(3,4,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The ubiquitous Euclidean distance는 두 벡터의 거리의 제곱의 제곱근으로 구한다.\n",
    "{x i } and {y i }, of the same size:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./figures/euclidean.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction \u001b[36meuclidean\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def euclidean[T <% Double, U <% Double]\n",
    "    (x: Array[T], y: Array[U]): Double =\n",
    "        Math.sqrt((x, y).zipped.foldLeft(0.0)((s, t) => { val d = t._1 - t._2; s + d*d} ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The cosine distance는 두 벡터의 코사인 각도로 정의된다. {x i } and {y i },\n",
    "of the same size:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./figures/cosine.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosine[T <% Double, U <% Double](xs: Array[T], ys: Array[T]): Double = {\n",
    "    val zeros = (0.0, 0.0, 0.0)\n",
    "    val sss = (xs, ys).zipped.foldLeft(zeros)(\n",
    "        {\n",
    "        (s, t) => \n",
    "            (s._1 + t._2 * t.3, s._2 + t._2 * t._2, s._3 + t._3* t._3)\n",
    "        }\n",
    "    )\n",
    "    sss._1 / Math.sqrt(sss._2 * sss._3)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "scalar의 dot product는 엄청 일반적인 경우이고 scala에서는 zip을 통해서 dot product를 구현한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "Compilation Failed",
      "\u001b[31mMain.scala:50: not found: value f",
      "    Array[Double] = x.zip(y).map( x => f(x._1, x._2) )",
      "                                       ^\u001b[0m"
     ]
    }
   ],
   "source": [
    "def dot (x:Array[Double], y:Array[Double]): \n",
    "    Array[Double] = x.zip(y).map( x => f(x._1, x._2) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction \u001b[36mdot\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def dot(x:Array[Double], y:Array[Double]): \n",
    "    Array[Double] = (x, y).zipped map ( _ * _)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of the K-means algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "the K-means 알고리즘의 큰 이점은 유사성이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K clusters {C k } \n",
    "\n",
    "means {m k }. \n",
    "\n",
    "The K-means 알고리즘은 최적화 문제라서 목적 자체가 재구조를 최소화하고 전체 오류를 줄이는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./figures/clustering.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "반복 알고리즘의단계는 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Initialize the centroids or means m k of the K clusters.\n",
    "* Assign observations to the nearest cluster given m k .\n",
    "* Iterate until no observations are reassigned to a cluster:\n",
    "  * Compute centroids m k that minimize the total error reconstruction for the current assignment\n",
    "  * Reassign the observations given the new centroids mk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 – cluster configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "K개의 군집 수 정하기와 군집내 센트로이드 초기화 작업이 필요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The first step is to define a cluster. A cluster is defined by the following parameters:\n",
    "* Centroid: center\n",
    "* The indices of the observations that belong to this cluster: members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36mscala.collection.mutable._\u001b[0m\n",
       "defined \u001b[32mtype \u001b[36mDblVector\u001b[0m\n",
       "defined \u001b[32mclass \u001b[36mCluster\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import scala.collection.mutable._\n",
    "\n",
    "type DblVector = Array[Double]\n",
    "\n",
    "class Cluster[T <% Double](val center: DblVector) {\n",
    "    val members = new ListBuffer[Int]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "클러스터는 이터레이션 계산에서 멤버를 관리한다. 한번에 두개 클러스터에 존재하지 못한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mobject \u001b[36mCluster\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "object Cluster {\n",
    "    def apply[T <% Double](c:DblVector):Cluster[T] = new Cluster[T](c)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "클러스터는 관측치의 멤버를 관리하거나 갱신할 수 있고 관측 멤버의 분산과 표준 편차를 구할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "SyntaxError: found \"): Cluster[T] ={\\n   \", expected (\",\" ~ Type | \"]\") in",
      "def moveCenter(xt: XTSeries[Array[T]): Cluster[T] ={",
      "                                    ^"
     ]
    }
   ],
   "source": [
    "type XTSeries[T] = Array[T]\n",
    "\n",
    "def += (n:Int): Unit = members.append(n)\n",
    "def moveCenter(xt: XTSeries[Array[T]): Cluster[T] ={\n",
    "    val sums = members.map(xt(_).map(_.toDouble)).\n",
    "        toList.transpose.map( _.sum)    \n",
    "    Cluster[T](sums.map( _ / members.size).toArray)\n",
    "}\n",
    "def stdDev(xt: XTSeries[Array[T]], \n",
    "           distance: (DblVector, Array[T]) => Double): Double =\n",
    "{\n",
    "    Stats[Double](members.map(xt( _)).\n",
    "        map( distance(center, _)).toArray).stdDev\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "세가지 중요한 함수가 있다.\n",
    "\n",
    "* += : 클러스터에 멤버 추가\n",
    "* moveCenter : 기존 멤버들에서 새로운 센트로이드를 계산하여 새로운 클러스터를 준다.\n",
    "* stdDev : 표준 편차나 밀집도를 계산해 준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cluster selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "관측치를 재배정할 때 클러스터를 선택하는 방법들이 있다. 표준 편차가 크고 밀도가 낮은 넘을 선택하게 되는데 우리는 멤버가 많은 크러스터를 대안으로 선택할 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Defining K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "incomplete"
     ]
    }
   ],
   "source": [
    "class KMeans[T <% Double](K: Int, maxIters: Int, \n",
    "        distance: (DblVector,Array[T]) => Double)\n",
    "        (implicit order: Ordering[T], m: Manifest[T]) extends\n",
    "            PipeOperator[XTSeries[Array[T]], List[Cluster[T]]] {\n",
    "    def |> : PartialFunction[XTSeries[Array[T]], List[Cluster[T]]]\n",
    "    def initialize(xt:XTSeries[Array[T]]): List[Cluster[T]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "다른 데이터 처리처럼 K means도 파이프 |>를 사용한다. 2장에서 배운 종소성 주입이 클러스터에 통합되었다. 초기화 함수로 센트로이드가 설정된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "센트로이드의 초기화는 K means의 빠른 수렴을 위해서 중요하다. 센트로이드 후보를 적합을 평가하는 유전자 알고리즘 군에 속한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "초기화 과정:\n",
    "\n",
    "1. Compute the standard deviation of the set of observations.\n",
    "2. Select the dimension k {x k , 0 , x k , 1 ... x k , n } with maximum standard deviation.\n",
    "3. Rank the observations by their increasing value of standard deviation for the dimension k.\n",
    "4. Divide the ranked observations set equally into K sets {S m }.\n",
    "5. Find the median values, size (S m )/2.\n",
    "6. Use the corresponding observations as centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "Compilation Failed",
      "\u001b[31mMain.scala:151: not found: type Cluster",
      "                def initialize(xt:XTSeries[Array[T]]): List[Cluster[T]]={",
      "                                                            ^\u001b[0m",
      "\u001b[31mMain.scala:151: not found: type XTSeries",
      "                def initialize(xt:XTSeries[Array[T]]): List[Cluster[T]]={",
      "                                  ^\u001b[0m",
      "\u001b[31mMain.scala:152: not found: value statistics",
      "    val stats = statistics(xt) //1",
      "                ^\u001b[0m",
      "\u001b[31mMain.scala:153: diverging implicit expansion for type Ordering[B]",
      "starting with method Tuple9 in object Ordering",
      "    val maxSDevDim = Range(0,stats.size).maxBy (stats( _ ).stdDev)//2",
      "                                               ^\u001b[0m",
      "\u001b[31mMain.scala:160: not found: value K",
      "    Range(0, K).foldLeft(List[Cluster[T]]())((xs, i) => Cluster[T](centroids(i)) :: xs) //7",
      "             ^\u001b[0m",
      "\u001b[31mMain.scala:160: not found: type Cluster",
      "    Range(0, K).foldLeft(List[Cluster[T]]())((xs, i) => Cluster[T](centroids(i)) :: xs) //7",
      "                              ^\u001b[0m",
      "\u001b[31mMain.scala:160: not found: type T",
      "    Range(0, K).foldLeft(List[Cluster[T]]())((xs, i) => Cluster[T](centroids(i)) :: xs) //7",
      "                                                                ^\u001b[0m"
     ]
    }
   ],
   "source": [
    "def initialize(xt:XTSeries[Array[T]]): List[Cluster[T]]={\n",
    "    val stats = statistics(xt) //1\n",
    "    val maxSDevDim = Range(0,stats.size).maxBy (stats( _ ).stdDev)//2\n",
    "    val rankedObs = xt.zipWithIndex\n",
    "    .map(x=> (x._1(maxSDevDim), x._2)) //2\n",
    "    .sortWith( _._1 < _._1) //3\n",
    "    val halfSegSize = ((rankedObs.size>>1)/K).floor.toInt //4\n",
    "    val centroids = rankedObs.filter(isContained( _, halfSegSize,\n",
    "    rankedObs.size) ).map(n => xt(n._2)) //6\n",
    "    Range(0, K).foldLeft(List[Cluster[T]]())((xs, i) => Cluster[T](centroids(i)) :: xs) //7\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "초기화에는 Agha-Ashour 알고리즘이 쓰였다.\n",
    "\n",
    "http://www.mecs-press.org/ijisa/ijisa-v4-n1/IJISA-V4-N1-3.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "incomplete"
     ]
    }
   ],
   "source": [
    "def isContained(t: (T,Int), hSz: Int, dim: Int): Boolean =\n",
    "    ((t._2 % hSz == 0) && (t._2 %(hSz<<1) != 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2 – cluster assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "두번째 단계는 클러스터에 관측치를 배정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "Compilation Failed",
      "\u001b[31mMain.scala:151: not found: type XTSeries",
      "                def assignToClusters(xt: XTSeries[Array[T]], ",
      "                                         ^\u001b[0m",
      "\u001b[31mMain.scala:152: not found: type Cluster",
      "                     clusters: List[Cluster[T]], ",
      "                                    ^\u001b[0m"
     ]
    }
   ],
   "source": [
    "def assignToClusters(xt: XTSeries[Array[T]], \n",
    "                     clusters: List[Cluster[T]], \n",
    "                     membership: Array[Int]): Int = {\n",
    "    xt.toArray\n",
    "        .zipWithIndex\n",
    "        .filter(x => { //1\n",
    "        val nearestCluster = getNearestCluster(clusters, x._1)//2\n",
    "        val reassigned = nearestCluster != membership(x._2)\n",
    "        clusters(nearestCluster) += x._2 //3\n",
    "        membership(x._2) = nearestCluster //4\n",
    "        reassigned\n",
    "    }).size\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "배정의 핵심은 가장 가까운 클러스터의 인덱스를 계산하는 필터에 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "최근거리를 계산하는 메소드:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "incomplete"
     ]
    }
   ],
   "source": [
    "def getNearestCluster(clusters: List[Cluster[T]], x:Array[T]): Int={\n",
    "    clusters.zipWithIndex.foldLeft((Double.MaxValue,0))((p,c) => {\n",
    "    val measure = distance(c._1.center, x)\n",
    "    if(measure < p._1) (measure, c._2) else p\n",
    "    })._2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3 – iterative reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "마지막 단계는 재구축 오류를 계산하는 반복자를 구현하는 것이다.\n",
    "\n",
    "K means 클러스터 알고리즘에서는 파이프 |>에 반복자 추출이 포함되어 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "SyntaxError: found \" ... }\\n        } //4\", expected (\"}\" | `case`) in",
      "            case None => { ... }",
      "                          ^"
     ]
    }
   ],
   "source": [
    "def |> :PartialFunction[XTSeries[Array[T]], List[Cluster[T]]] = {\n",
    "    case xt: XTSeries[Array[T]] if(xt.size>2 && xt(0).size>0) => {\n",
    "        val clusters = initialize(xt) //1\n",
    "        if( clusters.isEmpty) List.empty\n",
    "        else {\n",
    "            val membership = Array.fill(xt.size)(0)\n",
    "            val reassigned = assignToClusters(xt,clusters,membership)//2\n",
    "            var newClusters: List[Cluster[T]] = List.empty\n",
    "            Range(0, maxIters).find( _ => {\n",
    "                newClusters = clusters.map( c => {\n",
    "                    if( c.size > 0) c.moveCenter(xt, dimension(xt))\n",
    "                    else clusters.filter( _.size > 0)\n",
    "                        .maxBy( _.stdDev(xt, distance))\n",
    "                    }) //3\n",
    "                assignToClusters(xt, newClusters, membership) == 0\n",
    "            }) match {\n",
    "            case Some(index) => newClusters\n",
    "            case None => { ... }\n",
    "        } //4\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### K-means algorithm exit condition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "드물지만 알고리즘은 동일한 관측치를 재배정하면서 가끔 수렵하지 못하게 막을 수 도 있다.\n",
    "\n",
    "그래서 이터레이션의 최대치를 정하여 탈출 조건을 추가하기 권한다. 만일 K-means가 최대 반복에도 수렴하지 못하면 첨부터 다시해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "Compilation Failed",
      "\u001b[31mMain.scala:154: not found: type KMeans",
      "    (implicit order: Ordering[T], m: Manifest[T]): KMeans[T] = ",
      "                                                   ^\u001b[0m",
      "\u001b[31mMain.scala:155: not found: type KMeans",
      "    new KMeans[T](K, maxIters, euclidean) ; def stdDev[T](c: List[Cluster[T]], xt: XTSeries[Array[T]]): ",
      "        ^\u001b[0m",
      "\u001b[31mMain.scala:155: missing arguments for method euclidean in class $user;",
      "follow this method with `_' if you want to treat it as a partially applied function",
      "    new KMeans[T](K, maxIters, euclidean) ; def stdDev[T](c: List[Cluster[T]], xt: XTSeries[Array[T]]): ",
      "                               ^\u001b[0m",
      "\u001b[31mMain.scala:155: not found: type Cluster",
      "    new KMeans[T](K, maxIters, euclidean) ; def stdDev[T](c: List[Cluster[T]], xt: XTSeries[Array[T]]): ",
      "                                                                  ^\u001b[0m",
      "\u001b[31mMain.scala:155: not found: type XTSeries",
      "    new KMeans[T](K, maxIters, euclidean) ; def stdDev[T](c: List[Cluster[T]], xt: XTSeries[Array[T]]): ",
      "                                                                                   ^\u001b[0m"
     ]
    }
   ],
   "source": [
    "def apply[T <% Double](K: Int, maxIters: Int)\n",
    "    (implicit order: Ordering[T], m: Manifest[T]): KMeans[T] = \n",
    "    new KMeans[T](K, maxIters, euclidean)\n",
    "\n",
    "def stdDev[T](c: List[Cluster[T]], xt: XTSeries[Array[T]]): \n",
    "    List[Double] = c.map( _.stdDev(xt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Centroid versus mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "센트로이드와 평균은 값은 개체를 참조한다. 이제부터는 둘을 혼용해서 사용할 거다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "ordering a trait와 Manifest는 apply 생성자에서 제공된다. 왜서냐면? 클라이언트 코드에서 런타임에 제공 받는단 보장이 없기에"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curse of dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "의미 있는 피처의 수를 위해서는 모델은 거대 관측치를 필요로 한다.\n",
    "\n",
    "50보다 작은 매우 작은 수의 관측치로 구성된 K means 클러스터링은 상당히 편의된 모델을 만들어 낸다.\n",
    "\n",
    "저자는 경험상 다음의 규칙을 사용한단다.\n",
    "훈련 집합:n, 기대 클러스터: K, 피처: N\n",
    "\n",
    "n < K.N."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dimensionality versus size of training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "모델의 차원대 관측치의 수에 따른 이슈는 비지도학습 알고리즘에서는 특별하지 않다. 지도 학습에서는 viable training plan을 구축해서 동일한 과제를 대한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src='./figures/PriceModelForKMeansClustering.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "제한을 두거나 관측치를 줄인다.\n",
    "\n",
    "* Sampling the trading data without losing a significant amount of information from the raw data, assuming the distribution of observations follows a known probability density function.\n",
    "* Smoothing the data to remove the noise as seen in Chapter 3, Data Preprocessing, assuming the noise is Gaussian. In our test, a smoothing technique will remove the price outliers for each stock and therefore reduce the number of features (trading session). This approach differs from the sampling approach because it does not require an assumption that the dataset follows a known density function. On the other hand, the reduction of features will be less significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "이런 접근법이 이 책의 목적상 최고의 솔루션이라 할 수 있다. 하지만 실제 상업용 분석에서는 권하지 않는다. 이후에 소개될 PCA가 신뢰할 수 있는 차원 축소 기법중에 하나이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "13년 1월에서 12월까지 일별 주식 종가의 클러스터를 뽑는 것이 목적이다. 127개 종목은 S&P 500에서 무작위로 추출됐고 아래 시각화했다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./figures/PriceActionOfStocksUsedInKMeansClustering.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "핵심은 클러스터링에 앞서 적절한 피처를 정하는 것이다. 전체 가격 이력을 고려하여 252일치를 피처로 선정했다. 하지만 관측치 수는 전체 가격 범위를 확인하기 위해서는 너무 제한적이다. 50개의 관측치는 80일에서 130일 사이의 주식 종가이다. 일별 종가는 민,맥스를 사용해서 정규화했다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "먼저, K means 알고리즘을 실행하는 간단한 함수를 정의하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "Compilation Failed",
      "\u001b[31mMain.scala:155: not found: value Val",
      "Val MAX_ITERS = 150",
      "^\u001b[0m",
      "\u001b[31mMain.scala:157: not found: type DblMatrix",
      " ; def run(K: Int, obs: DblMatrix): Unit = {",
      "                        ^\u001b[0m",
      "\u001b[31mMain.scala:158: not found: value KMeans",
      "    val kmeans = KMeans[Double](K, MAX_ITERS) //1",
      "                 ^\u001b[0m",
      "\u001b[31mMain.scala:158: not found: value MAX_ITERS",
      "    val kmeans = KMeans[Double](K, MAX_ITERS) //1",
      "                                   ^\u001b[0m"
     ]
    }
   ],
   "source": [
    "Val MAX_ITERS = 150\n",
    "def run(K: Int, obs: DblMatrix): Unit = {\n",
    "    val kmeans = KMeans[Double](K, MAX_ITERS) //1\n",
    "    val clusters = kmeans |> XTSeries[DblVector](obs) //2\n",
    "    clusters.foreach( _.center.foreach( show( _ ))) //3\n",
    "    clusters.map( _.stdDev(XTSeries[DblVector](obs, euclidean))).foreach( show( _ ) )\n",
    "    //4\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The KMeans class is first initialized with a number of clusters, K, and a maximum number of iterations, MAX_ITERS (line 1). These two parameters are domain and problem specific.\n",
    "The clustering algorithm is executed (line 2) returning a list of clusters. The clusters'centroid information is then displayed (line 3) and the standard deviation is computed for each of the clusters for a given number of clusters, K, and observations, obs (line 4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's load the data from CSV files using the DataSource class (refer to the Data extraction section in Appendix A, Basic Concepts):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "SyntaxError: found \" = { ... }\\n    case \", expected (`=>` | `⇒`) in",
      "    case Some(noPrice) = { ... }",
      "                      ^"
     ]
    }
   ],
   "source": [
    "final val path = \"resources/data/chap4/\"\n",
    "val extractor = YahooFinancials.adjClose :: List[Array[String] =>Double]() // 5\n",
    "def symbols = DataSource.listSymbols(path) //6\n",
    "final val START = 80\n",
    "final val SAMPLES = 50\n",
    "val normalize=true\n",
    "val prices = symbols.map(s =>DataSource(s,path,normalize) |> extractor) //7\n",
    "prices.find(_.isEmpty) match {\n",
    "    //8\n",
    "    case Some(noPrice) = { ... }\n",
    "    case None => {\n",
    "        val values = prices. map(x => x(0))\n",
    "        .map(_.drop(START).take(SAMPLES))\n",
    "        args.map(_.toInt) foreach( run(_, values)) //9\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "클러스터 분석은 80일~130일 종가에 적용했다. YahooFinancials을 사용해서 추출했다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "K=3일 때 군집 분석이다. 각 클러스터의 평균 벡터는 아래 그림과 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./figures/ChartOfMeansOfClustersUsingKMeansK3.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "세개의 클러스터의 평균벡터는 확연히 구분된다. 맨위와 맨 아래는 1, 2번이다. sd가 0.34와 0.27로 매우 유사한 패턴을 보인다. 1과 2의 요소간에 차이는 거의 0.37이다. 클러스터 3은 시작시점에는 2번과 유사하게 나오지만 뒤로 갈 수록 3번에 가까운 패턴을 보인다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "시간 윈도와 거래 기간 (80~130)에서 동작을 쉽게 설명해 준다. 연방 준비의 정책을 그대로 반영하고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./figures/ListOfStocks.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "클러스터 수에 따른 영향도를 평가해 보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning the number of clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "2에서 15까지 클러스터 수를 조절할 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "2부터 시작해 보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./figures/ChartOfMeansOfClustersUsingKMeansK2.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "라벨 2는 3개 클러스터의 라벨 3와 비슷하다. 하지만 라벨 1은 3개 클러스터의 라벨 1, 2를 통합한 것과 비슷하다.\n",
    "\n",
    "통합 효과는 왜 클러스터 1의 표준 편차인 0.55가 클러스터 2의 표준편차 0.28에 두배가 되는지 보여 준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "이번에 5개 클러스터이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./figures/ChartsOfMeansOfClustersUsingKMeansK5.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "이번 차트에서 클러스터 1, 2, 3는 3개 클러스터의 동일한 라벨의 클러스터와 비슷하다. 벡터 4인 클러스터는 클러스터 3과 유사하지만 방향이 뒤집혔다. 다시 말해 3, 4 클러서트는 금융정책에 반대로 반응한 사례이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "10개 클러스터 짜리이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./figures/ChartOfMeansOfClustersUsingKMeansK10.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "k=3일 때 나온 본 클러스터 1,2,3의 평균은 여전히 나오고 있다. 가장 신뢰할 수 있는 클러스터로 가정할 수 있다. 이 클러스터는 낮은 표준 편차와 높은 밀도를 가진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "centroid c j인 cluster C j의 밀도를 유클리디안 거리의 역수로 정의할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./figures/DistanceOfEuclidean.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "이번엔 13개 이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./figures/BarChartOfTheAverageClusterDensityForK1to13.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "기대했던바와 같이 평균 밀도는 K가 증가하면 따라서 증가한다. K=5일 때까지는 의미 있게 증가하다가 이후에는 항상 증가하지는 않는다.\n",
    "\n",
    "비정상적인 3개 요일을 설명하자면:\n",
    "* The original data is noisy\n",
    "* The model is somewhat dependent on the initialization of the centroids\n",
    "* The exit condition is too loose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "클러스터의 품질은 F1으로 측정된다. F1이 높은 모델을 찾아 정한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "결과 품질에 영향을 주는 조절 파라미터를 되돌아 보면:\n",
    "\n",
    "* Initial selection of centroid\n",
    "* Number of K clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "일부에선 유사성 기준은 명료성과 클러스터 밀도에 영향을  줄 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "최종적으로 중요한 고려사항은 K means의 계산 복잡도이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "이런 장점에더 불과하고 결측치나 미관측치 정보는 다룰수 없고 피처는 서로에게 영향을 미친다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Expectation-maximization (EM) algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "EM은 maximum likelihood를 추정하기 위해 소개돼었다. 관측되지 않은 값이 있는 우도를 최대화 하는 모델의 피처를 계산하기 위함이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "반복 알고리즘은 다음으로 구성된다:\n",
    "\n",
    "* The expectation, E, of the maximum likelihood for the observed data by inferring the latent values (E-step)\n",
    "* The model features that maximize the expectation E (M-step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "EM 알고리즘은 표중 가우시안 분포를 따르는 클러스터 문제를 풀때 적용된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Gaussian mixture model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "잠복 변수 Z는 행위의 근본인 Z의 모델 X의 행위로 가시되할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./figures/VisualizationOfObservedAndLatentFeatures.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The mixture model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "{xi}가 잠복 피처 {zi}와 결함된 관찰 피처이면 z가 j로 주어졌을 때 피처 xi를 위한 확률은?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./figures/pxi.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "확률 p는 기저 분포라 한다. 전체 모델 θ= {x i , z k }로 확장하면 조건부 확률은 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./figures/pxisigma.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "가장 넓게 사용되는 믹스처 모델은 기저분포 P를 정규 분포로, 조건 확률을 가중 정규 다변량 분포로 표현할 수 있는 가우시안 믹스처 모델이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./figures/pxisigma-1.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EM overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "구현에 관한한 EM 아고리즘은 3단계를 거친다.\n",
    "\n",
    "* The computation of the log likelihood for the model features given some latent variables (LL).\n",
    "* The computation of the expectation of the log likelihood at iteration t (E-step).\n",
    "* The maximization of the expectation at iteration t (M-step)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* LL: 관차 변수 X = {xi}와 잠복 변수 Z={zi}를 고려하자. 주어진 z에 대한 x의 로그 라이클리후드는"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./figures/ll.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* E-step: 모델 변수 θ에서의 기대값은 다음으로 계산된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./figures/estep.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* M-step: 함수 Q는 모델 피처 θ를 위해서 최대화된다. Borman's tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src='./figures/mstep.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Tip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Inner workings of EM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "아파치 컴먼스에서 계산한다. 관심있음 보시길."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type EM = MultivariateNormalMixtureExpectationMaximization\n",
    "type EMOutput = List[(Double, DblVector, DblVector)]\n",
    "import scala.collections.JavaConversions._ //1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The constructor of the MultivariateEM class uses the standard template for machine learning algorithm classes:\n",
    "\n",
    "* Parameterized view bound type\n",
    "* Implementation of EM as a data transformation by extending PipeOperator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Here is an implementation of the constructor of MultivariateEM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MultivariateEM[T <% Double](K: Int) extends\n",
    "PipeOperator[XTSeries[Array[T]], EMOutput]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "자바 컬랙션을 스칼라 컬랙션으로 변환시켜 줘야 한다. (line 1).\n",
    "\n",
    "EM 알고리즘 구현은 |> 오퍼레이터가 필요하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "SyntaxError: found \")//6\\n    ....\", expected (\"}\" | `case`) in",
      "    ))//6",
      "     ^"
     ]
    }
   ],
   "source": [
    "def |> : PartialFunction[XTSeries[Array[T]], EMOutput] = {\n",
    "    case xt: XTSeries[Array[T]] if(xt.size>0 && dimension(xt)>0) =>{\n",
    "        val data: DblMatrix = xt\n",
    "        //2\n",
    "        val multivariateEM = new EM(data)\n",
    "        val est = MultivariateNormalMixtureExpectationMaximization\n",
    "            .estimate(data, K)\n",
    "        multivariateEM.fit(est) //3\n",
    "\n",
    "        val newMixture = multivariateEM.getFittedModel //4\n",
    "        val components = newMixture.getComponents.toList //5\n",
    "        components.map(p => (p.getKey.toDouble, p.getValue.getMeans,\n",
    "        p.getValue.getStandardDeviations)\n",
    "    ))//6\n",
    "    ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Third-party library exceptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "스칼라는 exception을 method 선언에 넣지 않는다. (java와 다름)\n",
    "로컬에서 예외가 처리된다는 보장이 없음. 이문제를 예외가 두개의 시나리오의 3rd 라이브러리에서 발생한다.\n",
    "\n",
    "* The documentation of the API does not list all the types of exceptions\n",
    "* The library is updated and a new type of exception is added to a method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가장 간단한 스칼라의 예외 처리 메커니즘은 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "SyntaxError: found \"    ..\\n} match {\\n   \", expected (\"}\" | `case`) in",
      "    ..",
      "^"
     ]
    }
   ],
   "source": [
    "Try {\n",
    "    ..\n",
    "} match {\n",
    "    case Success(results) => ...\n",
    "    case Failure(exception) => ...\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "주식에 EM을 적용해 보자.\n",
    "\n",
    "EM 알고리즘으로 분석할 주식의 수는 사용된 관측수를 제한한다. 간단한 옵션은 주식의 노이즈 중 일부를 거른다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Tip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering and sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "클러스터링 전에 Moving average와 고정 간격 표본법을 조합한 전처리는 매우 기초적인 것들이다. 예를 들어 과거의 주식 이력은 동일한 노이즈 성격을 보인다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val extractor = YahooFinancials.adjClose :: List[Array[String] =>Double]() //1\n",
    "val period = 8\n",
    "val samplingRate = 10\n",
    "val smAv = SimpleMovingAverage[Double](period) //2\n",
    "val obs = DataSource.listSymbols(path).map(sym => { //3\n",
    "    val xs = DataSource(sym, path, true) |> extractor //2\n",
    "    val values : XTSeries[Double] = XTSeries.|>(xs)).head //4\n",
    "    val filtered = smAv |> values\n",
    "    filtered.zipWithIndex //5\n",
    "        .drop(period+1).toArray //6\n",
    "        .filter( _._2%samplingRate==0)\n",
    "        .map( _._1)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./figures/ChartOfTheNormalizedMeansPerClusterUsingEMK3.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "클러스터 2, 3은 유사한 패턴을 가진다. 두개는 동일하거나 유사한 통찰을 보인다. 다음은 표준 정규 분포화한 그림이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./figures/ChartOfTheNormalizedStandardDeviationPerClusterUsingEMK3.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "각 클러스터의 평균 벡터의 표준 편차의 분포는 다른 하나의 분포가 연방 준비은행의 공지에 따라 상승할 때 두개 산업의 주가가 동반 하락한다는 사실을 설명해 준다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Relation to K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "You may wonder what is the relation between EM and K-means as both techniques address\n",
    "the same problem. The K-means algorithm assigns each observation uniquely to one and\n",
    "only one cluster. The EM algorithm assigns an observation based on posterior probability.\n",
    "K-means is a special case of the EM for Gaussian mixtures [4:11]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Online EM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Online learning is a powerful strategy for training a clustering model when dealing with\n",
    "very large datasets. This strategy has regained interest from scientists lately. The\n",
    "description of online EM is beyond the scope of this tutorial. However, you may need to\n",
    "know that there are several algorithms available for online EM if you ever have to deal with\n",
    "large datasets: batch EM, stepwise EM, incremental EM, and Monte Carlo EM\n",
    "[4:12]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Dimension reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "정보 영역의 사전 지식없이 데이터 과학자는 가능한 모든 피처를 포함하려 한다. 그것도 첫 분석에서, 결국에는 가정을 만들고 이는 열악하고 위험한 차원 축소를  하게된다.\n",
    "\n",
    "모델이 수백개의 피처를 사용하는게 비일반적이진 않다. 노이즈 필터링 기법은 피처에 대한 모델의 민감도를 떨어뜨린다. 하지만 이런 노이즈에 연관된 피처는 훈련 과정 전에는 알려저 있지 않아 버려질 수 없다. 결과적으로 모델 훈련은 매우 부담스럽게 된다.\n",
    "\n",
    "과적합은 거대한 피처 집합에는 또 다른 장애이다. 제한된 크기의 훈련 집합은 거대한 피처로 모델을 만드는 것을 허락하지 않는다.\n",
    "\n",
    "차원 축소 기술은 이런 문제를 해결한다.\n",
    "\n",
    "여기 차원 축소에 3가지 접근법이 있다.\n",
    "\n",
    "* Statistical analysis solutions such as ANOVA for smaller feature sets\n",
    "* Regularization and shrinking techniques, which are introduced in Chapter 6, Regression and Regularization\n",
    "* Algorithms that maximize the variance of the dataset by transforming the covariance matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal components analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "차원 축소의 목적은 변수의 차원을 줄여서  원본  피처를 새롭게 정렬된 피처로 변환하는 것이다. 원본 관측은 더 낮은 차원으로 변환된다.\n",
    "\n",
    "Let's consider a model with two features {x, y} and a set of observations {xi, yi} plotted on\n",
    "the following chart:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./figures/VisualizationOfPrincipalComponentsForA2DimensionModel.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "x,y는 더 적절히 매칭되는 X,Y로 변환된다. 설명력이 높은 변수를 first principal component라 불린다. n번쨰를 n principal component라 부른다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Lindsay Smith 의 강좌를 추천한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "PCA와 공분산 메트릭스는 두개의 피처 X, Y의 공분삭이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./figures/pca1.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "여기서 x,y에 대한 각각의 평균이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "공분산은 zScore로 계산된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./figures/pca2.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "n개 피처로 만든어 진 모델에서 공분산 메트릭스는 아래와 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./figures/pca3.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "x에서  X로 변환은 공분산 메트릭스의 아이젠벨류로 계산된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./figures/pca4.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "아이젠 벨류는 분산과 누적 분삭의 감소 차수로 rank된다. 마지막으로 m의 최상위 아이젠 벨류는 기 정의된 문턱값을 넘는 값이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./figures/pca5.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "알고리즘은 5단계로 구현된다.\n",
    "\n",
    "* Compute the zScore for the observations by standardizing the mean and standard deviation.\n",
    "* Compute the covariance matrix Σ for the original set of observations.\n",
    "* Compute the new covariance matrix Σ' for the observations with the transformed features by extracting the eigenvalues and eigenvectors.\n",
    "* Convert the matrix to rank eigenvalues by decreasing the order of variance. The ordered eigenvalues are the principal components.\n",
    "* Select the principal components for which the total sum of variance exceeds a threshold by as a percentage of the trace of the new covariance matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "공분사 메트릭스의 대각화에 의한 PC의 추출은 아래에 가시화된다. 공분산 수치를 위해 컬러가 들어 갔다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./figures/VisualizationOfTheExtractionOfEigenvaluesInPCA.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "아이젠벨류는 차원을 축소로 rank된다. PCA알고리즘은 누적 값이 무의미할때 종료된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "PCA는 Apache Commons Math lib에 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import types.ScalaMl._, types.CommonMath._, //2\n",
    "\n",
    "def |> : PartialFunction[XTSeries[Array[T]], (DblMatrix, DblVector)]={\n",
    "    case xt: XTSeries[Array[T]] if(xt !=null && xt.size>1) => {\n",
    "        zScoring(xt) match {//1\n",
    "    case Some(obs) => {\n",
    "        val covariance = new Covariance(obs).getCovarianceMatrix //3\n",
    "        val transf = new EigenDecomposition(covariance)\n",
    "        val eigVectors = transf.getV //4\n",
    "        val eigValues = new ArrayRealVector(transf.getRealEigenvalues)\n",
    "        val cov = obs.multiply(eigVectors).getData\n",
    "        (cov, eigValues.toArray) //5\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "PCA는 원 집합을 표준화해서 사용한다. 공분산을 구하고, 아이젠분석을 해서 벡터의 값을 반환한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Test case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "PCA알고리즘을 34개 재무 척도로 표현하면\n",
    "\n",
    "1. Trailing Price-to-Earnings ratio (PE)\n",
    "2. Price-to-Sale ratio (PS)\n",
    "3. Price-to-Book ratio (PB)\n",
    "4. Return on Equity (ROE)\n",
    "5. Operation Margin (OM)\n",
    "\n",
    "\n",
    "val data = Array[(String, DblVector)] (\n",
    "// Ticker\n",
    "PE\n",
    "PS\n",
    "PB\n",
    "ROE\n",
    "OM\n",
    "(\"QCOM\", Array[Double](20.8, 5.32, 3.65, 17.65,29.2)),\n",
    "(\"IBM\",\n",
    "Array[Double](13, 1.22, 12.2, 88.1,19.9)),\n",
    "...\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "PCA를 실행핳는 클라이언트 코드:\n",
    "\n",
    "val pca = new PCA[Double] //1\n",
    "val input = data.map( _._2.take(3))\n",
    "val cov = pca |> XTSeries[DblVector](input) //2\n",
    "Display.show(toString(cov), logger) //3\n",
    "\n",
    "pca생성, 데이터에서 pca 변환 수행하여 공분산 추출, 화면에 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "34개 재무 비율은 5차원 모델을 사용한다. 그 고유값은\n",
    "\n",
    "2.5321, 1.0350, 0.7438, 0.5218, 0.3284\n",
    "\n",
    "고유값에 상대적인 값은 아래 그림에 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./figures/DistributionOfEigenvaluesInPCAFor5Dimensions.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The chart shows that 3 out of 5 features account for 85 percent of total variance (trace of\n",
    "the transformed covariance matrix). I invite you to experiment with different combinations\n",
    "of these features. The selection of a subset of the existing features is as simple as applying\n",
    "Scala's take or drop methods:\n",
    "\n",
    "Val numFeatures = 4\n",
    "val ts = XTSeries[DblVector](data.map(_._2.take(numFeatures)))\n",
    "\n",
    "Let's plot the cumulative eigenvalues for the three different model configurations:\n",
    "* Five features: PE, PS, PB, ROE, and OM\n",
    "* Four features: PE, PS, PB, and ROE\n",
    "* Three features: PE, PS, and PB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./figures/DistributionOfEigenvaluesInPCAFor345Features.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The chart displays the cumulative value of eigenvalues that are the variance of the\n",
    "transformed features Xi. If we apply a threshold of 90 percent to the cumulative variance,\n",
    "then the number of principal components for each test model is as follows:\n",
    "* {PE, PS, PB}: 2\n",
    "* {PE, PS, PB, ROE}:3\n",
    "* {PE, PS, PB, ROE, OM}: 3\n",
    "\n",
    "In conclusion, the PCA algorithm reduced the dimension of the model by 33 percent for\n",
    "the 3-feature model, 25 percent for the 4-feature model, and 40 percent for the 5-feature\n",
    "model for a threshold of 90 percent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-validation of PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Like any other unsupervised learning technique, the resulting principal components have\n",
    "to be validated through a one or K-fold cross-validation using a regression estimator such\n",
    "as partial least square regression (PLSR) or the predicted residual error sum of\n",
    "squares (PRESS). For those not afraid of statistics, I recommend Fast Cross-validation\n",
    "in Robust PCA by S. Engelen and M. Hubert [4:14]. You need to be aware, however, that\n",
    "the implementation of these regression estimators is not simple.\n",
    "\n",
    "The principal components can be validated through a 1-fold or K-fold cross-validation, by\n",
    "performing some type of regression estimators or EM on the same dataset. The validation\n",
    "of the PCA is beyond the scope and space allocated to this chapter.\n",
    "\n",
    "Principal components analysis is a special case of the more general factor analysis. The\n",
    "later class of algorithm does not require the transformation of the covariance matrix to be\n",
    "orthogonal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other dimension reduction techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Although quite popular, the principal components analysis is far from being the only\n",
    "dimension reduction method. Here are some alternative techniques, listed as reference:\n",
    "factor analysis, principal factor analysis, maximum likelihood factor analysis, independent\n",
    "component analysis (ICA), Random projection, nonlinear PCA, nonlinear ICA, Kohonen's\n",
    "self-organizing maps, neural networks, and multidimensional scaling, just to name a few\n",
    "[4:15]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The three unsupervised learning techniques share the same limitation—a high\n",
    "computational complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The K-means has the computational complexity of O(iKnm), where i is the number of\n",
    "iterations, K the number of clusters, n the number of observations, and m the number of\n",
    "features. The algorithm can be improved through the use of other techniques by using the\n",
    "following techniques:\n",
    "* Reducing the average number of iterations by seeding the centroid using an\n",
    "algorithm such as initialization by ranking the variance of the initial cluster as\n",
    "described at the beginning of this chapter.\n",
    "* Using a parallel implementation of K-means and leveraging a large-scale framework\n",
    "such as Hadoop or Spark.\n",
    "* Reducing the number of outliers and possible features by filtering out the noise with\n",
    "a smoothing algorithm such as a discrete Fourier transform or a Kalman filter.\n",
    "* Decreasing the dimensions of the model by following a two-step process: a first pass\n",
    "with a smaller number of clusters K and/or a loose exit condition regarding the\n",
    "reassignment of data points. The data points close to each centroid are aggregated\n",
    "into a single observation. A second pass is then run on a smaller set of observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### EM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The computational complexity of the expectation-maximization algorithm for each\n",
    "iteration (E + M steps) is O(m 2 n), where m is the number of hidden or latent variables and\n",
    "n is the number of observations.\n",
    "\n",
    "A partial list of suggested performance improvement includes:\n",
    "* Filtering of raw data to remove noise and outliers\n",
    "* Using a sparse matrix on a large feature set to reduce the complexity of the\n",
    "covariance matrix, if possible\n",
    "* Applying the Gaussian mixture model (GMM) wherever possible: the\n",
    "assumption of Gaussian distribution simplifies the computation of the log likelihood\n",
    "* Using a parallel data processing framework such as Apache Hadoop or Spark as\n",
    "explained in the Apache Spark section in Chapter 12, Scalable Frameworks\n",
    "* Using a kernel method to reduce the estimate of covariance in the E-step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The computational complexity of the extraction of the principal components is O(m 2 n +\n",
    "n3), where m is the number of features and n the number of observations. The first term\n",
    "represents the computational complexity for computing the covariance matrix. The last\n",
    "term reflects the computational complexity of the eigenvalue decomposition.\n",
    "\n",
    "The list of potential performance improvements or alternative solutions for PCA includes:\n",
    "* Assuming that the variance is Gaussian\n",
    "* Using a sparse matrix to compute eigenvalues for problems with large feature sets\n",
    "and missing data\n",
    "* Investigating alternatives to PCA to reduce the dimension of a model such as the\n",
    "discrete Fourier transform (DFT) or singular value decomposition (SVD)\n",
    "[4:16]\n",
    "* Using the PCA in conjunction with EM (a research)\n",
    "* Deploying a dataset on a parallel data processing framework such as Apache Spark\n",
    "or Hadoop as explained in the Apache Spark section in Chapter 12, Scalable\n",
    "Frameworks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This completes the overview of three of the most commonly used unsupervised learning\n",
    "techniques:\n",
    "* K-means for clustering fully observed features of a model with reasonable\n",
    "dimensions\n",
    "* Expectation-maximization for clustering a combination of observed and latent\n",
    "features\n",
    "* Principal components analysis to transform and extract the most critical features in\n",
    "terms of variance\n",
    "\n",
    "The key point to remember is that unsupervised learning techniques are used:\n",
    "* By themselves to extract structures and associations from unlabelled observations\n",
    "* As a preprocessing stage to supervised learning in reducing the number of features\n",
    "prior to the training phase\n",
    "\n",
    "In the next chapter, we will address the second use case, and cover supervised learning\n",
    "techniques starting with generative models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Thank you so much, Q & A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.11",
   "language": "scala211",
   "name": "scala211"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": "scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "pygments_lexer": "scala",
   "version": "2.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
