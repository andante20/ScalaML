{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 9: Advanced Text Processing with Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 싸이그래머, 스사모 / 스칼라ML : 파트2 - SparkML [1]\n",
    "* 김무성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What's so special about text data?\n",
    "* Extracting the right features from your data\n",
    "* Using a TF-IDF model\n",
    "* Evaluating the impact of text processing\n",
    "* Word2Vec models\n",
    "* Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's so special about text data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Text data can be complex to work with for two main reasons. \n",
    "    - First, text and language have an inherent structure that is not easily captured using the raw words as is (for example, meaning, context, different types of words, sentence structure, and different languages, to highlight a few). \n",
    "    - Therefore, naïve feature extraction is usually relatively ineffective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Extracting the right features from your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* Term weighting schemes\n",
    "* Feature hashing\n",
    "* Extracting the TF-IDF features from the 20 Newsgroups dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### natural language processing (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter, we will focus on two feature extraction techniques available within MLlib: the TF-IDF term weighting scheme and feature hashing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term weighting schemes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* bag-of-words\n",
    "* term frequency-inverse document frequency (TF-IDF)\n",
    "* document\n",
    "* inverse document frequency\n",
    "* corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap9.1.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap9.2.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature hashing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature hashing is a technique to deal with high-dimensional data and is often used with text and categorical datasets where the features can take on many unique values (often many millions of values). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1-of-K feature encoding\n",
    "* hashing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there are two important drawbacks, which are as follows:\n",
    "\n",
    "* As we don't create a mapping of features to index values, we also cannot do the reverse mapping of feature index to value. This makes it harder to, for example, determine which features are most informative in our models.\n",
    "* As we are restricting the size of our feature vectors, we might experience hash collisions. This happens when two different features are hashed into the same index in our feature vector. Surprisingly, this doesn't seem to have a severe impact on model performance as long as we choose a reasonable feature vector dimension relative to the dimension of the input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Extracting the TF-IDF features from the 20 Newsgroups dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* Exploring the 20 Newsgroups data\n",
    "* Applying basic tokenization\n",
    "* Improving our tokenization\n",
    "* Removing stop words\n",
    "* Excluding terms based on frequency\n",
    "* A note about stemming\n",
    "* Training a TF-IDF model \n",
    "* Analyzing the TF-IDF weightings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate the concepts in this chapter, we will use a well-known text dataset called 20 Newsgroups; this dataset is commonly used for text-classification tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset splits up the available data into training and test sets that comprise\n",
    "60 percent and 40 percent of the original data, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!wget http://qwone.com/~jason/20Newsgroups/20news-bydate.tar.gz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!tar xfvz 20news-bydate.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20news-bydate-test   20news-bydate.tar.gz\t\t\t  figures\r\n",
      "20news-bydate-train  9_Advanced_Text_Processing_with_Spark.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alt.atheism\t\t  rec.autos\t      sci.space\r\n",
      "comp.graphics\t\t  rec.motorcycles     soc.religion.christian\r\n",
      "comp.os.ms-windows.misc   rec.sport.baseball  talk.politics.guns\r\n",
      "comp.sys.ibm.pc.hardware  rec.sport.hockey    talk.politics.mideast\r\n",
      "comp.sys.mac.hardware\t  sci.crypt\t      talk.politics.misc\r\n",
      "comp.windows.x\t\t  sci.electronics     talk.religion.misc\r\n",
      "misc.forsale\t\t  sci.med\r\n"
     ]
    }
   ],
   "source": [
    "!ls 20news-bydate-train/  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of files under each newsgroup folder; each file contains an individual message posting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52550  52605  52660  53565  53620  53675  53731  53786\t53841  53905  54058\r\n",
      "52551  52606  52661  53566  53621  53676  53732  53787\t53842  53906  54070\r\n",
      "52552  52607  52662  53567  53622  53677  53733  53788\t53843  53907  54071\r\n",
      "52553  52608  52663  53568  53623  53678  53734  53789\t53844  53908  54076\r\n",
      "52554  52609  52664  53569  53624  53679  53735  53790\t53845  53909  54079\r\n",
      "52555  52610  52665  53570  53625  53680  53736  53791\t53846  53910  54080\r\n",
      "52556  52611  52666  53571  53626  53681  53737  53792\t53847  53911  54094\r\n",
      "52557  52612  52667  53572  53627  53682  53738  53793\t53848  53912  54113\r\n",
      "52558  52613  52668  53573  53628  53683  53739  53794\t53849  53913  54117\r\n",
      "52559  52614  52669  53574  53629  53684  53740  53795\t53850  53914  54118\r\n",
      "52560  52615  53468  53575  53630  53685  53741  53796\t53851  53915  54122\r\n",
      "52561  52616  53521  53576  53631  53686  53742  53797\t53852  53916  54123\r\n",
      "52562  52617  53522  53577  53632  53687  53743  53798\t53853  53917  54124\r\n",
      "52563  52618  53523  53578  53633  53688  53744  53799\t53854  53918  54134\r\n",
      "52564  52619  53524  53579  53634  53689  53745  53800\t53855  53919  54158\r\n",
      "52565  52620  53525  53580  53635  53690  53746  53801\t53856  53920  54164\r\n",
      "52566  52621  53526  53581  53636  53691  53747  53802\t53857  53921  54165\r\n",
      "52567  52622  53527  53582  53637  53692  53748  53803\t53858  53922  54180\r\n",
      "52568  52623  53528  53583  53638  53693  53749  53804\t53859  53926  54181\r\n",
      "52569  52624  53529  53584  53639  53694  53750  53805\t53860  53927  54182\r\n",
      "52570  52625  53530  53585  53640  53695  53751  53806\t53861  53928  54183\r\n",
      "52571  52626  53531  53586  53641  53696  53752  53807\t53862  53929  54184\r\n",
      "52572  52627  53532  53587  53642  53697  53753  53808\t53863  53930  54185\r\n",
      "52573  52628  53533  53588  53643  53698  53754  53809\t53864  53931  54192\r\n",
      "52574  52629  53534  53589  53644  53699  53755  53810\t53865  53932  54193\r\n",
      "52575  52630  53535  53590  53645  53700  53756  53811\t53866  53933  54194\r\n",
      "52576  52631  53536  53591  53646  53701  53757  53812\t53867  53956  54195\r\n",
      "52577  52632  53537  53592  53647  53702  53758  53813\t53868  53959  54196\r\n",
      "52578  52633  53538  53593  53648  53703  53759  53814\t53869  53960  54197\r\n",
      "52579  52634  53539  53594  53649  53704  53760  53815\t53870  53961  54198\r\n",
      "52580  52635  53540  53595  53650  53705  53761  53816\t53871  53962  54199\r\n",
      "52581  52636  53541  53596  53651  53706  53762  53817\t53872  53963  54200\r\n",
      "52582  52637  53542  53597  53652  53707  53763  53818\t53873  53964  54218\r\n",
      "52583  52638  53543  53598  53653  53708  53764  53819\t53874  53965  54219\r\n",
      "52584  52639  53544  53599  53654  53709  53765  53820\t53875  53966  54220\r\n",
      "52585  52640  53545  53600  53655  53710  53766  53821\t53876  53967  54221\r\n",
      "52586  52641  53546  53601  53656  53711  53767  53822\t53877  53968  54222\r\n",
      "52587  52642  53547  53602  53657  53712  53768  53823\t53878  53969  54223\r\n",
      "52588  52643  53548  53603  53658  53713  53769  53824\t53881  53970  54241\r\n",
      "52589  52644  53549  53604  53659  53714  53770  53825\t53882  53971  54242\r\n",
      "52590  52645  53550  53605  53660  53715  53771  53826\t53883  53972  54252\r\n",
      "52591  52646  53551  53606  53661  53716  53772  53827\t53885  53973  54253\r\n",
      "52592  52647  53552  53607  53662  53717  53773  53828\t53887  53974  54254\r\n",
      "52593  52648  53553  53608  53663  53718  53774  53829\t53888  53975  54360\r\n",
      "52594  52649  53554  53609  53664  53719  53775  53830\t53889  53976  54361\r\n",
      "52595  52650  53555  53610  53665  53720  53776  53831\t53891  53977  54554\r\n",
      "52596  52651  53556  53611  53666  53721  53777  53832\t53892  53978  54555\r\n",
      "52597  52652  53557  53612  53667  53722  53778  53833\t53893  53979  54556\r\n",
      "52598  52653  53558  53613  53668  53724  53779  53834\t53894  53980  54723\r\n",
      "52599  52654  53559  53614  53669  53725  53780  53835\t53895  53981  55022\r\n",
      "52600  52655  53560  53615  53670  53726  53781  53836\t53896  53982\r\n",
      "52601  52656  53561  53616  53671  53727  53782  53837\t53897  53983\r\n",
      "52602  52657  53562  53617  53672  53728  53783  53838\t53899  54055\r\n",
      "52603  52658  53563  53618  53673  53729  53784  53839\t53903  54056\r\n",
      "52604  52659  53564  53619  53674  53730  53785  53840\t53904  54057\r\n"
     ]
    }
   ],
   "source": [
    "!ls 20news-bydate-train/rec.sport.hockey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: dchhabra@stpl.ists.ca (Deepak Chhabra)\r\n",
      "Subject: Superstars and attendance (was Teemu Selanne, was +/- leaders)\r\n",
      "Nntp-Posting-Host: stpl.ists.ca\r\n",
      "Organization: Solar Terresterial Physics Laboratory, ISTS\r\n",
      "Distribution: na\r\n",
      "Lines: 115\r\n",
      "\r\n",
      "\r\n",
      "Dean J. Falcione (posting from jrmst+8@pitt.edu) writes:\r\n",
      "[I wrote:]\r\n",
      "\r\n",
      ">>When the Pens got Mario, granted there was big publicity, etc, etc,\r\n",
      ">>and interest was immediately generated.  Gretzky did the same thing for LA. \r\n",
      ">>However, imnsho, neither team would have seen a marked improvement in\r\n",
      ">>attendance if the team record did not improve.  In the year before Lemieux\r\n",
      ">>came, Pittsburgh finished with 38 points.  Following his arrival, the Pens\r\n",
      ">>finished with 53, 76, 72, 81, 87, 72, 88, and 87 points, with a couple of\r\n",
      "                          ^^\r\n",
      ">>Stanley Cups thrown in.\r\n",
      "      \r\n"
     ]
    }
   ],
   "source": [
    "!head -20 20news-bydate-train/rec.sport.hockey/52550"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the 20 Newsgroups data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will again use Spark's wholeTextFiles method to read the content of each file into a record in our RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you will see the total record count, which should be the same as the preceding Total input paths to process screen output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11314\n"
     ]
    }
   ],
   "source": [
    "val path = \"./20news-bydate-train/*\"\n",
    "val rdd = sc.wholeTextFiles(path)\n",
    "val text = rdd.map { case (file, text) => text }\n",
    "println(text.count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will take a look at the newsgroup topics available:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the number of messages is roughly even between the topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(rec.sport.hockey,600)\n",
      "(soc.religion.christian,599)\n",
      "(rec.motorcycles,598)\n",
      "(rec.sport.baseball,597)\n",
      "(sci.crypt,595)\n",
      "(sci.med,594)\n",
      "(rec.autos,594)\n",
      "(sci.space,593)\n",
      "(comp.windows.x,593)\n",
      "(sci.electronics,591)\n",
      "(comp.os.ms-windows.misc,591)\n",
      "(comp.sys.ibm.pc.hardware,590)\n",
      "(misc.forsale,585)\n",
      "(comp.graphics,584)\n",
      "(comp.sys.mac.hardware,578)\n",
      "(talk.politics.mideast,564)\n",
      "(talk.politics.guns,546)\n",
      "(alt.atheism,480)\n",
      "(talk.politics.misc,465)\n",
      "(talk.religion.misc,377)\n"
     ]
    }
   ],
   "source": [
    "val newsgroups = rdd.map{ case (file, text) => file.split(\"/\").takeRight(2).head }\n",
    "val countByGroup = newsgroups.map(n => (n, 1)).reduceByKey(_ + _).collect.sortBy(-_._2).mkString(\"\\n\")\n",
    "println(countByGroup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying basic tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* tokens\n",
    "* tokenization\n",
    "* whitespace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by applying a simple whitespace tokenization, together with converting each token to lowercase for each document:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running this code snippet, you will see the total number of unique tokens after applying our tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "402978\n"
     ]
    }
   ],
   "source": [
    "val text = rdd.map{ case (file, text) => text }\n",
    "val whiteSpaceSplit = text.flatMap(t => t.split(\" \").map(_.toLowerCase))\n",
    "println(whiteSpaceSplit.distinct.count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at a randomly selected document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from:,jar2e@faraday.clas.virginia.edu,jar2e@faraday.clas.virginia.edu,re:,re:,israeli,terrorism\n",
      "organization:,virginia\n",
      "lines:,12\n",
      "\n",
      "would,document,reporters\"?,i,you,with,region,which,by,by,government,,be,the,the,human,human,rights,by\n",
      "sealing,off,strip,,get,the,palestinian-on-palestinian\n",
      "civil,and,behave,peace.,peace.,not,murtezaoglu)\n",
      "subject:,re:,it,could,shoot,henrik@quayle.kpc.com's,1993,1993,16:45:17,computer,science,science,article,henrik@quayle.kpc.com,to,drag,drag,azerbaijan.,a,a,capital,not,,not,,above,that,short,that,stop,for,anyone,it\n",
      "is,\n",
      "\n",
      ">the,have,from,given,azeris,stalin),are,are,conflict.,are,defending,defending,expect,forces,them\n",
      "within,their,that,turkey,turkey,\n",
      ">,crisis,occur,not,playing,a,deck,,,invade?\n",
      "are,the,with,header\n",
      "in,hopes,greek\n"
     ]
    }
   ],
   "source": [
    "println(whiteSpaceSplit.sample(true, 0.3, 42).take(100).mkString(\",\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving our tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do this by splitting each raw document on nonword characters using a regular expression pattern:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This reduces the number of unique tokens significantly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130126\n"
     ]
    }
   ],
   "source": [
    "val nonWordSplit = text.flatMap(t => t.split(\"\"\"\\W+\"\"\").map(_.toLowerCase))\n",
    "println(nonWordSplit.distinct.count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we inspect the first few tokens, we will see that we have eliminated most of the less useful characters in the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kv07,jejones,jejones,bone,bone,k29p,ml5,ratifi,valuemask,bruns,mmejv5,jsoh,bluffing,125215,p05,isgal,kjd0,kjd0,c1381,200649,pacified,itchy,ishbel,ishbel,c1,c5ckp9,gustafson,nonmeasurable,1qsj1d9,q9w,omnimovie,agm,personify,framestores,framestores,salvageable,10011100b,bippy,dolphin,102756,margitan,wp3d,cannibal,cannibal,bronfman,prescient,211053,211053,committeewoman,pmjh,jesuit,cscx_sy_,cscx_sy_,incomparable,6097,6097,204843,busbar,busbar,6jx,623,dbuck,nixdorf,omputers,eavesdroppers,arely,mortal,springer,perversity,interfere,nowadays,formac,maxscreens,khb,khb,fuenfzig,paradijs,cyclops,cyclops,sx,xtaddcallback,2_patch_version3,projector,1qmrdd,65e90h8y,65e90h8y,dib,rint69,antena,mcgiver,5297,kmv6snl,xwlr3hi,kipling,adventists,lanman,9mm,0hnz,lama,dxb132\n"
     ]
    }
   ],
   "source": [
    "println(nonWordSplit.distinct.sample(true, 0.3, 42).take(100).mkString(\",\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do this by applying another regular expression pattern and using this to filter out tokens that do not match the pattern:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This further reduces the size of the token set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84912\n"
     ]
    }
   ],
   "source": [
    "val regex = \"\"\"[^0-9]*\"\"\".r\n",
    "val filterNumbers = nonWordSplit.filter(token =>regex.pattern.matcher(token).matches)\n",
    "println(filterNumbers.distinct.count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at another random sample of the filtered tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jejones,hif,hif,glorifying,glorifying,wuair,feh,schwabam,valuemask,tough,_congressional,interlaced,husky,relieves,artur,entitlements,arax,arax,phillips,mirosoft,dwi,believiing,odf,odf,steaminess,pacified,hizbolah,wqod,adultery,urtfi,nauseam,ishbel,wout,emerich,emerich,seetex,viewed,milking,museum,eur,typeset,smits,twarda,twarda,salvageable,mget,sctc,sctc,bippy,cities,mmg,root_iden,root_iden,cannibal,michaelr,michaelr,mswin,gundry,gundry,diccon,babied,belated,borg,sham,trivialized,jlange,impute,octopi,seeing,dtn,kruislaan,awdprime,detergent,yan,yan,agenzia,icbz,volcanic,volcanic,conspiricy,chov,deadweight,gcs,ouzq,kjiv,kjiv,hour,caramate,dchhabra,springer,alchoholic,cherylm,perversity,chq,bruncati,interfere,amro,exhausting,murdering,khb\n"
     ]
    }
   ],
   "source": [
    "println(filterNumbers.distinct.sample(true, 0.3, 42).take(100).mkString(\",\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Stop words\n",
    "    - Examples of typical English stop words include and, but, the, of, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take a look at some of the tokens in our corpus that have the highest occurrence across all documents to get an idea about some other stop words to exclude:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code, we took the tokens after filtering out numeric characters and generated a count of the occurrence of each token across the corpus. We can now use Spark's top function to retrieve the top 20 tokens by count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(the,146532)\n",
      "(to,75064)\n",
      "(of,69034)\n",
      "(a,64195)\n",
      "(ax,62406)\n",
      "(and,57957)\n",
      "(i,53036)\n",
      "(in,49402)\n",
      "(is,43480)\n",
      "(that,39264)\n",
      "(it,33638)\n",
      "(for,28600)\n",
      "(you,26682)\n",
      "(from,22670)\n",
      "(s,22337)\n",
      "(edu,21321)\n",
      "(on,20493)\n",
      "(this,20121)\n",
      "(be,19285)\n",
      "(t,18728)\n"
     ]
    }
   ],
   "source": [
    "val tokenCounts = filterNumbers.map(t => (t, 1)).reduceByKey(_ + _)\n",
    "val oreringDesc = Ordering.by[(String, Int), Int](_._2)\n",
    "println(tokenCounts.top(20)(oreringDesc).mkString(\"\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we might expect, there are a lot of common words in this list that we could potentially label as stop words. Let's create a set of stop words with some of these as well as other common words. We will then look at the tokens after filtering out these stop words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ax,62406)\n",
      "(i,53036)\n",
      "(you,26682)\n",
      "(s,22337)\n",
      "(edu,21321)\n",
      "(t,18728)\n",
      "(m,12756)\n",
      "(subject,12264)\n",
      "(com,12133)\n",
      "(lines,11835)\n",
      "(can,11355)\n",
      "(organization,11233)\n",
      "(re,10534)\n",
      "(what,9861)\n",
      "(there,9689)\n",
      "(x,9332)\n",
      "(all,9310)\n",
      "(will,9279)\n",
      "(we,9227)\n",
      "(one,9008)\n"
     ]
    }
   ],
   "source": [
    "val stopwords = Set(\n",
    "     \"the\",\"a\",\"an\",\"of\",\"or\",\"in\",\"for\",\"by\",\"on\",\"but\", \"is\", \"not\",\n",
    "   \"with\", \"as\", \"was\", \"if\",\n",
    "     \"they\", \"are\", \"this\", \"and\", \"it\", \"have\", \"from\", \"at\", \"my\",\n",
    "   \"be\", \"that\", \"to\"\n",
    "   )\n",
    "val tokenCountsFilteredStopwords = tokenCounts.filter { case(k, v) => !stopwords.contains(k) }\n",
    "println(tokenCountsFilteredStopwords.top(20)(oreringDesc).mkString(\"\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One other filtering step that we will use is removing any tokens that are only one character in length. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ax,62406)\n",
      "(you,26682)\n",
      "(edu,21321)\n",
      "(subject,12264)\n",
      "(com,12133)\n",
      "(lines,11835)\n",
      "(can,11355)\n",
      "(organization,11233)\n",
      "(re,10534)\n",
      "(what,9861)\n",
      "(there,9689)\n",
      "(all,9310)\n",
      "(will,9279)\n",
      "(we,9227)\n",
      "(one,9008)\n",
      "(would,8905)\n",
      "(do,8674)\n",
      "(he,8441)\n",
      "(about,8336)\n",
      "(writes,7844)\n"
     ]
    }
   ],
   "source": [
    "val tokenCountsFilteredSize = tokenCountsFilteredStopwords.filter{ case (k, v) => k.size >= 2 }\n",
    "println(tokenCountsFilteredSize.top(20)(oreringDesc).mkString(\"\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excluding terms based on frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also a common practice to exclude terms during tokenization when their overall occurrence in the corpus is very low. For example, let's examine the least occurring terms in the corpus (notice the different ordering we use here to return the results sorted in ascending order):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tenex,1)\n",
      "(beckmans,1)\n",
      "(wuair,1)\n",
      "(feh,1)\n",
      "(ratifi,1)\n",
      "(schwabam,1)\n",
      "(bruns,1)\n",
      "(swith,1)\n",
      "(bluffing,1)\n",
      "(hif,1)\n",
      "(actu,1)\n",
      "(adnd,1)\n",
      "(wbp,1)\n",
      "(bunuel,1)\n",
      "(uncompression,1)\n",
      "(mxh,1)\n",
      "(_congressional,1)\n",
      "(fowl,1)\n",
      "(lennips,1)\n",
      "(jsoh,1)\n"
     ]
    }
   ],
   "source": [
    "val oreringAsc = Ordering.by[(String, Int), Int](-_._2)\n",
    "println(tokenCountsFilteredSize.top(20)(oreringAsc).mkString(\"\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there are many terms that only occur once in the entire corpus. Since typically we want to use our extracted features for other tasks such as document similarity or machine learning models, tokens that only occur once are not useful to learn from, as we will not have enough training data relative to these tokens. We can apply another filter to exclude these rare tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(mmg,2)\n",
      "(wexler,2)\n",
      "(theoreticians,2)\n",
      "(glorifying,2)\n",
      "(michaelr,2)\n",
      "(relieves,2)\n",
      "(_lwo,2)\n",
      "(isgal,2)\n",
      "(prescient,2)\n",
      "(eoeun,2)\n",
      "(mswin,2)\n",
      "(kielbasa,2)\n",
      "(bronfman,2)\n",
      "(defiance,2)\n",
      "(contoler,2)\n",
      "(omnimovie,2)\n",
      "(disobeyers,2)\n",
      "(historicity,2)\n",
      "(congresswoman,2)\n",
      "(relatifs,2)\n"
     ]
    }
   ],
   "source": [
    "val rareTokens = tokenCounts.filter{ case (k, v) => v < 2 }.map {case (k, v) => k }.collect.toSet\n",
    "val tokenCountsFilteredAll = tokenCountsFilteredSize.filter { case(k, v) => !rareTokens.contains(k) }\n",
    "println(tokenCountsFilteredAll.top(20)(oreringAsc).mkString(\"\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's count the number of unique tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51801\n"
     ]
    }
   ],
   "source": [
    "println(tokenCountsFilteredAll.count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now combine all our filtering logic into one function, which we can apply to each document in our RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(line: String): Seq[String] = {\n",
    "     line.split(\"\"\"\\W+\"\"\")\n",
    "       .map(_.toLowerCase)\n",
    "       .filter(token => regex.pattern.matcher(token).matches)\n",
    "       .filterNot(token => stopwords.contains(token))\n",
    "       .filterNot(token => rareTokens.contains(token))\n",
    "       .filter(token => token.size >= 2)\n",
    "       .toSeq\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check whether this function gives us the same result with the following code snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51801\n"
     ]
    }
   ],
   "source": [
    "println(text.flatMap(doc => tokenize(doc)).distinct.count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can tokenize each document in our RDD as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WrappedArray(faraday, clas, virginia, edu, virginia, gentleman, subject, re, israeli, terrorism, organization, university, virginia, lines, would, asking, too, much, you, document)\n"
     ]
    }
   ],
   "source": [
    "val tokens = text.map(doc => tokenize(doc))\n",
    "println(tokens.first.take(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note about stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A common step in text processing and tokenization is stemming. This is the conversion of whole words to a base form (called a word stem). \n",
    "    - stemming\n",
    "    - base form\n",
    "    - word stem\n",
    "* For example, plurals might be converted to singular (dogs becomes dog), and forms such as walking and walker might become walk. \n",
    "* Stemming can become quite complex and is typically handled with specialized NLP or search engine software \n",
    "    - such as \n",
    "        - NLTK, \n",
    "        - OpenNLP, and \n",
    "        - Lucene, \n",
    "* We will ignore stemming for the purpose of our example here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a TF-IDF model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now use MLlib to transform each document, in the form of processed tokens, into a vector representation. \n",
    "\n",
    "* The first step will be to use the HashingTF implementation, \n",
    "    which makes use of feature hashing to map each token in the input text to an index in the vector of term frequencies.\n",
    "* Then, we will compute the global IDF and \n",
    "* use it to transform the term frequency vectors into TF-IDF vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will import the classes we need and create our HashingTF instance, passing in a dim dimension parameter. While the default feature dimension is 2^20 (or around 1 million), we will choose 2^18 (or around 260,000), since with about 50,000 tokens, we should not experience a significant number of hash collisions, and a smaller dimension will be more memory and processing friendly for illustrative purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.linalg.{ SparseVector => SV }\n",
    "import org.apache.spark.mllib.feature.HashingTF\n",
    "import org.apache.spark.mllib.feature.IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[45] at map at HashingTF.scala:78"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dim = math.pow(2, 18).toInt\n",
    "val hashingTF = new HashingTF(dim)\n",
    "val tf = hashingTF.transform(tokens)\n",
    "tf.cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The transform function of HashingTF maps each input document (that is, a sequence of tokens) to an MLlib Vector. \n",
    "* We will also call cache to pin the data in memory to speed up subsequent operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the first element of our transformed dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "262144\n",
      "67\n",
      "WrappedArray(1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0)\n",
      "WrappedArray(3635, 5740, 9972, 19841, 24877, 29853, 33239, 39411, 47026, 49378)\n"
     ]
    }
   ],
   "source": [
    "val v = tf.first.asInstanceOf[SV]\n",
    "println(v.size)\n",
    "println(v.values.size)\n",
    "println(v.values.take(10).toSeq)\n",
    "println(v.indices.take(10).toSeq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can see that the dimension of each sparse vector of term frequencies is 262,144 (or 218 as we specified). \n",
    "* However, the number on non-zero entries in the vector is only 706. \n",
    "* The last two lines of the output show the frequency counts and vector indexes for the first few entries in the vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now compute the inverse document frequency for each term in the corpus by creating a new IDF instance and calling fit with our RDD of term frequency vectors as the input. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will then transform our term frequency vectors to TF-IDF vectors through the transform function of IDF:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67\n",
      "WrappedArray(0.3723902347580378, 2.736738856180986, 4.891233301577321, 5.462683547159747, 6.435984865169208, 6.561295835827856, 3.414990703794491, 3.9725923923582127, 5.807524033451476, 6.038047692063309)\n",
      "WrappedArray(3635, 5740, 9972, 19841, 24877, 29853, 33239, 39411, 47026, 49378)\n"
     ]
    }
   ],
   "source": [
    "val idf = new IDF().fit(tf)\n",
    "val tfidf = idf.transform(tf)\n",
    "val v2 = tfidf.first.asInstanceOf[SV]\n",
    "println(v2.values.size)\n",
    "println(v2.values.take(10).toSeq)\n",
    "println(v2.indices.take(10).toSeq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can see that the number of non-zero entries hasn't changed (at 706), nor have the vector indices for the terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing the TF-IDF weightings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's investigate the TF-IDF weighting for a few terms to illustrate the impact of the commonality or rarity of a term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we can compute the minimum and maximum TF-IDF weights across the entire corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.0,66155.39470409753)\n"
     ]
    }
   ],
   "source": [
    "val minMaxVals = tfidf.map { v =>\n",
    "     val sv = v.asInstanceOf[SV]\n",
    "     (sv.values.min, sv.values.max)\n",
    "}\n",
    "val globalMinMax = minMaxVals.reduce { case ((min1, max1),\n",
    "   (min2, max2)) =>\n",
    "     (math.min(min1, min2), math.max(max1, max2))\n",
    "}\n",
    "println(globalMinMax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As we can see, the minimum TF-IDF is zero, while the maximum is significantly larger:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF weighting will tend to assign a lower weighting to common terms. To see this, we can compute the TF-IDF representation for a few of the terms that appear in the list of top occurrences that we previously computed, such as you, do, and we:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WrappedArray(0.9965359935704624, 1.3348773448236835, 0.5457486182039175)\n"
     ]
    }
   ],
   "source": [
    "val common = sc.parallelize(Seq(Seq(\"you\", \"do\", \"we\")))\n",
    "val tfCommon = hashingTF.transform(common)\n",
    "val tfidfCommon = idf.transform(tfCommon)\n",
    "val commonVector = tfidfCommon.first.asInstanceOf[SV]\n",
    "println(commonVector.values.toSeq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If we form a TF-IDF vector representation of this document, we would see the values assigned to each term. Note that because of feature hashing, we are not sure exactly which term represents what. However, the values illustrate that the weighting applied to these terms is relatively low:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's apply the same transformation to a few less common terms that we might intuitively associate with being more linked to specific topics or concepts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WrappedArray(5.3265513728351666, 5.308532867332488, 5.483736956357579)\n"
     ]
    }
   ],
   "source": [
    "val uncommon = sc.parallelize(Seq(Seq(\"telescope\", \"legislation\", \"investment\")))\n",
    "val tfUncommon = hashingTF.transform(uncommon)\n",
    "val tfidfUncommon = idf.transform(tfUncommon)\n",
    "val uncommonVector = tfidfUncommon.first.asInstanceOf[SV]\n",
    "println(uncommonVector.values.toSeq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Using a TF-IDF model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* Document similarity with the 20 Newsgroups dataset and TF-IDF features\n",
    "* Training a text classifier on the 20 Newsgroups dataset using TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF weighting is often used as a preprocessing step for other models, such as dimensionality reduction, classification, or regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate the potential uses of TF-IDF weighting, we will explore two examples. \n",
    "\n",
    "* The first is using the TF-IDF vectors to compute document similarity, \n",
    "* while the second involves training a multilabel classification model with the TF-IDF vectors as input features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document similarity with the 20 Newsgroups dataset and TF-IDF features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuitively, we might expect two documents to be more similar to each other if they share many terms. Conversely, we might expect two documents to be less similar\n",
    "if they each contain many terms that are different from each other. As we compute cosine similarity by computing a dot product of the two vectors and each vector\n",
    "is made up of the terms in each document, we can see that documents with a high overlap of terms will tend to have a higher cosine similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, we might expect two randomly chosen messages from the hockey newsgroup to be relatively similar to each other. Let's see if this is the case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val hockeyText = rdd.filter { case (file, text) => file.contains(\"hockey\") }\n",
    "val hockeyTF = hockeyText.mapValues(doc =>hashingTF.transform(tokenize(doc)))\n",
    "val hockeyTfIdf = idf.transform(hockeyTF.map(_._2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our hockey document vectors, we can select two of these vectors at random and compute the cosine similarity between them (as we did earlier, we will use Breeze for the linear algebra functionality, in particular converting our MLlib vectors to Breeze SparseVector instances first):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05028503626731515\n"
     ]
    }
   ],
   "source": [
    "import breeze.linalg._\n",
    "val hockey1 = hockeyTfIdf.sample(true, 0.1, 42).first.asInstanceOf[SV]\n",
    "val breeze1 = new SparseVector(hockey1.indices, hockey1.values,hockey1.size)\n",
    "val hockey2 = hockeyTfIdf.sample(true, 0.1, 43).first.asInstanceOf[SV]\n",
    "val breeze2 = new SparseVector(hockey2.indices, hockey2.values,hockey2.size)\n",
    "val cosineSim = breeze1.dot(breeze2) / (norm(breeze1) * norm(breeze2))\n",
    "println(cosineSim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By contrast, we can compare this similarity score to the one computed between one of our hockey documents and another document chosen randomly from the comp.graphics newsgroup, using the same methodology:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.008518345400584703\n"
     ]
    }
   ],
   "source": [
    "val graphicsText = rdd.filter { case (file, text) => file.contains(\"comp.graphics\") }\n",
    "val graphicsTF = graphicsText.mapValues(doc => hashingTF.transform(tokenize(doc)))\n",
    "val graphicsTfIdf = idf.transform(graphicsTF.map(_._2))\n",
    "val graphics = graphicsTfIdf.sample(true, 0.1, 42).first.asInstanceOf[SV]\n",
    "val breezeGraphics = new SparseVector(graphics.indices,graphics.values, graphics.size)\n",
    "val cosineSim2 = breeze1.dot(breezeGraphics) / (norm(breeze1) * norm(breezeGraphics))\n",
    "println(cosineSim2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, it is likely that a document from another sports-related topic might be more similar to our hockey document than one from a computer-related topic. However, we would probably expect a baseball document to not be as similar as our hockey document. Let's see whether this is the case by computing the similarity between a random message from the baseball newsgroup and our hockey document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.006477216225017649\n"
     ]
    }
   ],
   "source": [
    "val baseballText = rdd.filter { case (file, text) => file.contains(\"baseball\") }\n",
    "val baseballTF = baseballText.mapValues(doc => hashingTF.transform(tokenize(doc)))\n",
    "val baseballTfIdf = idf.transform(baseballTF.map(_._2))\n",
    "val baseball = baseballTfIdf.sample(true, 0.1, 42).first.asInstanceOf[SV]\n",
    "val breezeBaseball = new SparseVector(baseball.indices,baseball.values, baseball.size)\n",
    "val cosineSim3 = breeze1.dot(breezeBaseball) / (norm(breeze1) * norm(breezeBaseball))\n",
    "println(cosineSim3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Indeed, as we expected, we found that the baseball and hockey documents have a cosine similarity of 0.05, which is significantly higher than the comp.graphics document, but also somewhat lower than the other hockey document:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a text classifier on the 20 Newsgroups dataset using TF-ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using TF-IDF vectors, we expected that the cosine similarity measure would capture the similarity between documents, based on the overlap of terms between them. In a similar way, we would expect that a machine learning model, such as a classifier, would be able to learn weightings for individual terms; this would allow it to distinguish between documents from different classes. That is, it should be possible to learn a mapping between the presence (and weighting) of certain terms and a specific topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the 20 Newsgroups example, each newsgroup topic is a class, and we can train a classifier using our TF-IDF transformed vectors as input.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are dealing with a multiclass classification problem, we will use the naïve Bayes model in MLlib, which supports multiple classes. As the first step, we will import the Spark classes that we will be using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.regression.LabeledPoint\n",
    "import org.apache.spark.mllib.classification.NaiveBayes\n",
    "import org.apache.spark.mllib.evaluation.MulticlassMetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will need to extract the 20 topics and convert them to class mappings. We can do this in exactly the same way as we might for 1-of-K feature encoding, by assigning a numeric index to each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[80] at map at <console>:61"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val newsgroupsMap = newsgroups.distinct.collect().zipWithIndex.toMap\n",
    "val zipped = newsgroups.zip(tfidf)\n",
    "val train = zipped.map { case (topic, vector) => LabeledPoint(newsgroupsMap(topic), vector) }\n",
    "train.cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In the preceding code snippet, we took the newsgroups RDD, where each element is the topic, and used the zip function to combine it with each element in our tfidf RDD of TF-IDF vectors.\n",
    "* We then mapped over each key-value element in our new zipped RDD and created a LabeledPoint instance, where label is the class index and features is the TF-IDF vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an input RDD in the correct form, we can simply pass it to the naïve Bayes train function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val model = NaiveBayes.train(train, lambda = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the performance of the model on the test dataset. We will load the raw test data from the 20news-bydate-test directory, again using wholeTextFiles to read each message into an RDD element. We will then extract the class labels from the file paths in the same way as we did for the newsgroups RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val testPath = \"./20news-bydate-test/*\"\n",
    "val testRDD = sc.wholeTextFiles(testPath)\n",
    "val testLabels = testRDD.map { case (file, text) => \n",
    "    val topic = file.split(\"/\").takeRight(2).head\n",
    "    newsgroupsMap(topic)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val testTf = testRDD.map { case (file, text) => hashingTF.transform(tokenize(text)) }\n",
    "val testTfIdf = idf.transform(testTf)\n",
    "val zippedTest = testLabels.zip(testTfIdf)\n",
    "val test = zippedTest.map { case (topic, vector) => LabeledPoint(topic, vector) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Compile Error\n",
       "Message: <console>:12: error: not found: value test\n",
       "       val predictionAndLabel = test.map(p => (model.predict(p.features),p.label))\n",
       "                                ^\n",
       "StackTrace: "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictionAndLabel = test.map(p => (model.predict(p.features),p.label))\n",
    "val accuracy = 1.0 * predictionAndLabel.filter(x => x._1 == x._2).count() / test.count()\n",
    "val metrics = new MulticlassMetrics(predictionAndLabel)\n",
    "println(accuracy)\n",
    "println(metrics.weightedFMeasure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Evaluating the impact of text processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* Comparing raw features with processed TF-IDF features on the 20 Newsgroups dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the impact of applying these processing techniques by comparing the performance of a model trained on raw text data with one trained on processed and TF-IDF weighted text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing raw features with processed TF-IDF features on the 20 Newsgroups dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will simply apply the hashing term frequency transformation to the raw text tokens obtained using a simple whitespace splitting of the document text. We will train a model on this data and evaluate the performance on the test set as we did for the model trained with TF-IDF features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val rawTokens = rdd.map { case (file, text) => text.split(\" \") }\n",
    "val rawTF = texrawTokenst.map(doc => hashingTF.transform(doc))\n",
    "val rawTrain = newsgroups.zip(rawTF).map { case (topic, vector) =>\n",
    "   LabeledPoint(newsgroupsMap(topic), vector) }\n",
    "val rawModel = NaiveBayes.train(rawTrain, lambda = 0.1)\n",
    "val rawTestTF = testRDD.map { case (file, text) =>\n",
    "   hashingTF.transform(text.split(\" \")) }\n",
    "val rawZippedTest = testLabels.zip(rawTestTF)\n",
    "val rawTest = rawZippedTest.map { case (topic, vector) =>\n",
    "   LabeledPoint(topic, vector) }\n",
    "val rawPredictionAndLabel = rawTest.map(p =>\n",
    "   (rawModel.predict(p.features), p.label))\n",
    "val rawAccuracy = 1.0 * rawPredictionAndLabel.filter(x => x._1 ==\n",
    "   x._2).count() / rawTest.count()\n",
    "println(rawAccuracy)\n",
    "val rawMetrics = new MulticlassMetrics(rawPredictionAndLabel)\n",
    "println(rawMetrics.weightedFMeasure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  This is also partly a reflection of the fact that the naïve Bayes model is well suited to data in the form of raw frequency counts:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Word2Vec models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* Word2Vec on the 20 Newsgroups dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### distributed vector representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Word2Vec refers to a specific implementation of one of these models, often referred to as distributed vector representations. \n",
    "* The MLlib model uses a skip-gram model, which seeks to learn vector representations that take into account the contexts in which words occur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec on the 20 Newsgroups dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a Word2Vec model in Spark is relatively simple. We will pass in an RDD where each element is a sequence of terms. We can use the RDD of tokenized documents we have already created as input to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.feature.Word2Vec\n",
    "val word2vec = new Word2Vec()\n",
    "word2vec.setSeed(42)\n",
    "val word2vecModel = word2vec.fit(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once trained, we can easily find the top 20 synonyms for a given term (that is, the most similar term to the input term, computed by cosine similarity between the word vectors). For example, to find the 20 most similar terms to hockey, use the following lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vecModel.findSynonyms(\"hockey\", 20).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vecModel.findSynonyms(\"legislation\", 20).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 참고자료"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* [1] book - https://www.packtpub.com/big-data-and-business-intelligence/machine-learning-spark\n",
    "* [2] jypyter/all-spark-notebook docker - https://hub.docker.com/r/jupyter/all-spark-notebook/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.10.4",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
